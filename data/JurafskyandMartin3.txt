Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ⃝c 2019. All rights reserved. Draft of October 2, 2019.
 CHAPTER
3
N-gram Language Models
“You are uniformly charming!” cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for.
Random sentence generated from a Jane Austen trigram model
Predicting is difficult—especially about the future, as the old quip goes. But how about predicting something that seems much easier, like the next few words someone is going to say? What word, for example, is likely to follow
Please turn your homework ...
Hopefully, most of you concluded that a very likely word is in, or possibly over, but probably not refrigerator or the. In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word. The same models will also serve to assign a probability to an entire sentence. Such a model, for example, could predict that the following sequence has a much higher probability of appearing in a text:
     all of a sudden I notice three guys standing on the sidewalk
than does this same set of words in a different order:
     on guys all I of notice sidewalk three a sudden standing the
Why would you want to predict upcoming words, or assign probabilities to sen- tences? Probabilities are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition. For a speech recognizer to realize that you said I will be back soonish and not I will be bassoon dish, it helps to know that back soonish is a much more probable sequence than bassoon dish. For writing tools like spelling correction or grammatical error correction, we need to find and correct errors in writing like Their are two midterms, in which There was mistyped as Their, or Everything has improve, in which improve should have been improved. The phrase There are will be much more probable than Their are, and has improved than has improve, allowing us to help users by detecting and correcting these errors.
Assigning probabilities to sequences of words is also essential in machine trans- lation. Suppose we are translating a Chinese source sentence:
他 向 记者 介绍了 主要 内容 He to reporters introduced main content
As part of the process we might have built the following set of potential rough English translations:
he introduced reporters to the main contents of the statement he briefed to reporters the main contents of the statement
he briefed reporters on the main contents of the statement
2 CHAPTER 3 • N-GRAM LANGUAGE MODELS
AAC
language model
LM n-gram
A probabilistic model of word sequences could suggest that briefed reporters on is a more probable English phrase than briefed to reporters (which has an awkward to after briefed) or introduced reporters to (which uses a verb that is less fluent English in this context), allowing us to correctly select the boldfaced sentence above.
Probabilities are also important for augmentative and alternative communi- cation systems (Trnka et al. 2007, Kane et al. 2017). People often use such AAC devices if they are physically unable or sign but can instead using eye gaze or other specific movements to select words from a menu to be spoken by the system. Word prediction can be used to suggest likely words for the menu.
Models that assign probabilities to sequences of words are called language mod- els or LMs. In this chapter we introduce the simplest model that assigns probabilities to sentences and sequences of words, the n-gram. An n-gram is a sequence of N words: a 2-gram (or bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word se- quence of words like “please turn your”, or “turn your homework”. We’ll see how to use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire sequences. In a bit of terminological ambiguity, we usually drop the word “model”, and thus the term n- gram is used to mean either the word sequence itself or the predictive model that assigns it a probability. In later chapters we’ll introduce more sophisticated language models like the RNN LMs of Chapter 9.
3.1 N-Grams
Let’s begin with the task of computing P(w|h), the probability of a word w given some history h. Suppose the history h is “its water is so transparent that” and we want to know the probability that the next word is the:
P(the|its water is so transparent that). (3.1)
One way to estimate this probability is from relative frequency counts: take a very large corpus, count the number of times we see its water is so transparent that, and count the number of times this is followed by the. This would be answering the question “Out of the times we saw the history h, how many times was it followed by the word w”, as follows:
P(the|its water is so transparent that) = C(its water is so transparent that the)
C(its water is so transparent that)
(3.2)
 With a large enough corpus, such as the web, we can compute these counts and estimate the probability from Eq. 3.2. You should pause now, go to the web, and compute this estimate for yourself.
While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web isn’t big enough to give us good estimates in most cases. This is because language is creative; new sentences are created all the time, and we won’t always be able to count entire sentences. Even simple extensions of the example sentence may have counts of zero on the web (such as “Walden Pond’s water is so transparent that the”; well, used to have counts of zero).

bigram
The bigram model, for example, approximates the probability of a word given all the previous words P(wn|wn−1) by using only the conditional probability of the
3.1 • N-GRAMS 3
Similarly, if we wanted to know the joint probability of an entire sequence of words like its water is so transparent, we could do it by asking “out of all possible sequences of five words, how many of them are its water is so transparent?” We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences. That seems rather a lot to estimate!
For this reason, we’ll need to introduce cleverer ways of estimating the proba-
bility of a word w given a history h, or the probability of an entire word sequence W .
Let’s start with a little formalizing of notation. To represent the probability of a par-
ticular random variable Xi taking on the value “the”, or P(Xi = “the”), we will use
the simplification P(the). We’ll represent a sequence of N words either as w1 . . . wn
or wn (so the expression wn−1 means the string w1,w2,...,wn−1). For the joint prob- 11
ability of each word in a sequence having a particular value P(X = w1,Y = w2,Z = w3,...,W = wn) we’ll use P(w1,w2,...,wn).
Now how can we compute probabilities of entire sequences like P(w1 , w2 , ..., wn )? One thing we can do is decompose this probability using the chain rule of proba- bility:
P(X1...Xn) = P(X1)P(X2|X1)P(X3|X12)...P(Xn|Xn−1) 1
n
= 􏰃P(Xk|Xk−1)
(3.3)
(3.4)
1 k=1
Applying the chain rule to words, we get
P(wn) = P(w )P(w |w )P(w |w2)...P(w |wn−1)
112131n1 n
= 􏰃P(wk|wk−1) 1
k=1
The chain rule shows the link between computing the joint probability of a se-
quence and computing the conditional probability of a word given previous words.
Equation 3.4 suggests that we could estimate the joint probability of an entire se-
quence of words by multiplying together a number of conditional probabilities. But
using the chain rule doesn’t really seem to help us! We don’t know any way to
compute the exact probability of a word given a long sequence of preceding words,
P(wn|wn−1). As we said above, we can’t just estimate by counting the number of 1
times every word occurs following every long string, because language is creative and any particular context might have never occurred before!
The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words.
1
preceding word P(wn|wn−1). In other words, instead of computing the probability
P(the|Walden Pond’s water is so transparent that) (3.5) we approximate it with the probability
P(the|that) (3.6)

4 CHAPTER 3 • N-GRAM LANGUAGE MODELS
When we use a bigram model to predict the conditional probability of the next
Markov
n-gram
word, we are thus making the following approximation:
P(wn|wn−1) ≈ P(wn|wn−1) (3.7) 1
The assumption that the probability of a word depends only on the previous word is called a Markov assumption. Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past. We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the n-gram (which looks n − 1 words into the past).
Thus, the general equation for this n-gram approximation to the conditional probability of the next word in a sequence is
P(wn|wn−1) ≈ P(wn|wn−1 ) (3.8) 1 n−N+1
Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:
n
P(wn1) ≈ 􏰃P(wk|wk−1) (3.9)
k=1
How do we estimate these bigram or n-gram probabilities? An intuitive way to
estimate probabilities is called maximum likelihood estimation or MLE. We get the MLE estimate for the parameters of an n-gram model by getting counts from a
1
maximum likelihood estimation
normalize
corpus, and normalizing the counts so that they lie between 0 and 1.
For example, to compute a particular bigram probability of a word y given a previous word x, we’ll compute the count of the bigram C(xy) and normalize by the
sum of all the bigrams that share the same first word x: C(wn−1wn)
P(wn|wn−1) = 􏰁w C(wn−1w) (3.10)
 We can simplify this equation, since the sum of all bigram counts that start with a given word wn−1 must be equal to the unigram count for that word wn−1 (the reader should take a moment to be convinced of this):
P(wn|wn−1) = C(wn−1wn) (3.11) C(wn−1)
Let’s work through an example using a mini-corpus of three sentences. We’ll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word. We’ll also need a special end-symbol. </s>2
        <s> I am Sam </s>
        <s> Sam I am </s>
        <s> I do not like green eggs and ham </s>
1 For probabilistic models, normalizing means dividing by some total count so that the resulting prob- abilities fall legally between 0 and 1.
2 We need the end-symbol to make the bigram grammar a true probability distribution. Without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one. This model
would define an infinite set of probability distributions, with one distribution per sentence length. See Exercise 3.5.

relative frequency
Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the
observed frequency of a particular sequence by the observed frequency of a prefix.
This ratio is called a relative frequency. We said above that this use of relative
frequencies as a way to estimate probabilities is an example of maximum likelihood
estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood
of the training set T given the model M (i.e., P(T|M)). For example, suppose the
word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.
What is the probability that a random word selected from some other text of, say,
a million words will be the word Chinese? The MLE of its probability is 400 1000000
or .0004. Now .0004 is not the best possible estimate of the probability of Chinese occurring in all situations; it might turn out that in some other corpus or context Chinese is a very unlikely word. But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus. We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3.4.
Let’s move on to some examples from a slightly larger corpus than our 14-word example above. We’ll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some text- normalized sample user queries (a sample of 9332 sentences is on the website):
can you tell me about any good cantonese restaurants close by mid priced thai food is what i’m looking for
tell me about chez panisse
can you give me a listing of the kinds of food that are available i’m looking for a good place to eat breakfast
when is caffe venezia open during the day
Figure 3.1 shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project. Note that the majority of the values are zero. In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse.
Figure 3.2 shows the bigram probabilities after normalization (dividing each cell in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of unigram probabilities):
i wantto eat
2533 927 2417 746 Here are a few other useful probabilities:
P(i|<s>) = 0.25 P(english|want) = 0.0011 P(food|english) = 0.5 P(</s>|food) = 0.68
Now we can compute the probability of sentences like I want English food or I want Chinese food by simply multiplying the appropriate bigram probabilities to- gether, as follows:
3.1 • N-GRAMS 5
Here are the calculations for some of the bigram probabilities from this corpus P(I|<s>) = 23 = .67 P(Sam|<s>) = 13 = .33 P(am|I) = 23 = .67 P(</s>|Sam) = 21 = 0.5 P(Sam|am) = 12 = .5 P(do|I) = 13 = .33
For the general case of MLE n-gram parameter estimation: C(wn−1 wn)
P(wn|wn−1 ) = n−N+1 n−N+1 C(wn−1
n−N +1
)
(3.12)
   chinese
food lunch
 spend
 158
1093 341
 278

6
CHAPTER 3 • N-GRAM LANGUAGE MODELS
  i want to eat chinese food lunch spend
 i 5 827 0 9 0 0 0 2 want 2 0 608 1 6 6 5 1 to 2 0 4 686 2 0 6 211 eat 0 0 2 0 16 2 42 0 chinese 1 0 0 0 0 82 1 0 food 15 0 15 0 1 4 0 0 lunch 2 0 0 0 0 1 0 0 spend 1 0 1 0 0 0 0 0
  Figure 3.1
Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau- rant Project corpus of 9332 sentences. Zero counts are in gray.
 i want to eat chinese food lunch spend
 i 0.002 0.33 want 0.0022 0
to 0.00083 0 eat 0 0 chinese 0.0063 0 food 0.014 0 lunch 0.0059 0 spend 0.0036 0
0 0.0036 0
0 0 0.00079 0.0065 0.0054 0.0011 0 0.0025 0.087 0.0027 0.056 0
0.52 0.0063 0 0.0037 0 0 0.0029 0 0 0 0 0
0.66 0.0011 0.0017 0.28 0.0027 0
0 0 0.014 0
0 0 0.0036 0
0.0065 0.00083 0.021
0 0.00092 0
0
  Figure 3.2
 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus of 9332 sentences. Zero probabilities are in gray.
P(<s> i want english food </s>)
= P(i|<s>)P(want|i)P(english|want)
P(food|english)P(</s>|food) = .25×.33×.0011×0.5×0.68
= .000031
We leave it as Exercise 3.2 to compute the probability of i want chinese food.
What kinds of linguistic phenomena are captured in these bigram statistics? Some of the bigram probabilities above encode some facts that we think of as strictly syntactic in nature, like the fact that what comes after eat is usually a noun or an adjective, or that what comes after to is usually a verb. Others might be a fact about the personal assistant task, like the high probability of sentences beginning with the words I. And some might even be cultural rather than linguistic, like the higher probability that people are looking for Chinese versus English food.
Some practical issues: Although for pedagogical purposes we have only described trigram bigram models, in practice it’s more common to use trigram models, which con- 4-gram dition on the previous two words rather than the previous word, or 4-gram or even 5-gram 5-gram models, when there is sufficient training data. Note that for these larger n- grams, we’ll need to assume extra context for the contexts to the left and right of the sentence end. For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i.e., P(I|<s><s>). We always represent and compute language model probabilities in log format as log probabilities. Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes. Multiplying enough n-grams together would result in numerical underflow. By using log probabilities instead of raw probabilities, we get numbers that are not as small.
log probabilities

3.2
3.2 • EVALUATING LANGUAGE MODELS 7
Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them. The result of doing all computation and storage in log space is that we only need to convert back into probabilities if we need to report them at the end; then we can just take the exp of the logprob:
p1×p2×p3×p4 =exp(logp1+logp2+logp3+logp4) (3.13) Evaluating Language Models
extrinsic evaluation
intrinsic evaluation
training set
test set held out
The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in a component is really going to help the task at hand. Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.
Unfortunately, running big NLP systems end-to-end is often very expensive. In- stead, it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that mea- sures the quality of a model independent of any application.
For an intrinsic evaluation of a language model we need a test set. As with many of the statistical models in our field, the probabilities of an n-gram model come from the corpus it is trained on, the training set or training corpus. We can then measure the quality of an n-gram model by its performance on some unseen data called the test set or test corpus. We will also sometimes call test sets and other datasets that are not in our training sets held out corpora because we hold them out from the training data.
So if we are given a corpus of text and want to compare two different n-gram models, we divide the data into training and test sets, train the parameters of both models on the training set, and then compare how well the two trained models fit the test set.
But what does it mean to “fit the test set”? The answer is simple: whichever model assigns a higher probability to the test set—meaning it more accurately predicts the test set—is a better model. Given two probabilistic models, the better model is the one that has a tighter fit to the test data or that better predicts the details of the test data, and hence will assign a higher probability to the test data.
Since our evaluation metric is based on test set probability, it’s important not to let the test sentences into the training set. Suppose we are trying to compute the probability of a particular “test” sentence. If our test sentence is part of the training corpus, we will mistakenly assign it an artificially high probability when it occurs in the test set. We call this situation training on the test set. Training on the test set introduces a bias that makes the probabilities all look too high, and causes huge inaccuracies in perplexity, the probability-based metric we introduce below.
Sometimes we use a particular test set so often that we implicitly tune to its characteristics. We then need a fresh test set that is truly unseen. In such cases, we call the initial test set the development test set or, devset. How do we divide our data into training, development, and test sets? We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible. At the minimum, we would want to pick
development test

8 CHAPTER 3 • N-GRAM LANGUAGE MODELS
perplexity
the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models. In practice, we often just divide our data into 80% training, 10% development, and 10% test. Given a large corpus that we want to divide into training and test, test data can either be taken from some continuous sequence of text inside the corpus, or we can remove smaller “stripes” of text from randomly selected parts of our corpus and combine them into a test set.
3.2.1 Perplexity
In practice we don’t use raw probability as our metric for evaluating language mod- els, but a variant called perplexity. The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words. For a test set W = w1w2 ...wN,:
we get:
i=1
􏰇􏰆􏰆􏰅N 􏰃N 1
1
N
=N1 P(w1w2 ...wN)
We can use the chain rule to expand the probability of W :
􏰇􏰆􏰆􏰅N 􏰃N 1
PP(W) = P(w w ...w )− 12N
􏰄
(3.14)
   PP(W) =
Thus, if we are computing the perplexity of W with a bigram language model,
(3.15)
P(wi|wi−1) (3.16)
 P(wi|w1...wi−1)
 PP(W) =
 Note that because of the inverse in Eq. 3.15, the higher the conditional probabil- ity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is equivalent to maximizing the test set probability according to the language model. What we generally use for word sequence in Eq. 3.15 or Eq. 3.16 is the entire se- quence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin- and end-sentence markers <s> and </s> in the probability computation. We also need to include the end-of-sentence marker </s> (but not the beginning-of-sentence marker <s>) in the total count of word to- kens N.
There is another way to think about perplexity: as the weighted average branch- ing factor of a language. The branching factor of a language is the number of possi- ble next words that can follow any word. Consider the task of recognizing the digits in English (zero, one, two,..., nine), given that (both in some training set and in some
1
test set) each of the 10 digits occurs with equal probability P = 10 . The perplexity of
this mini-language is in fact 10. To see that, imagine a test string of digits of length N, and assume that in the training set all the digits occurred with equal probability. By Eq. 3.15, the perplexity will be
i=1

But suppose that the number zero is really frequent and occurs far more often than other numbers. Let’s say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i.e. has a high probability. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller. We leave this exact calculation as exercise 12.
We see in Section 3.7 that perplexity is also closely related to the information- theoretic notion of entropy.
Finally, let’s look at an example of how perplexity can be used to compare dif- ferent n-gram models. We trained unigram, bigram, and trigram grammars on 38 million words (including start-of-sentence tokens) from the Wall Street Journal, us- ing a 19,979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words with Eq. 3.16. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars.
As we see above, the more information the n-gram gives us about the word sequence, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related inversely to the likelihood of the test sequence according to the model).
Note that in computing perplexities, the n-gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artificially low. The perplexity of two language models is only comparable if they use identical vocabularies.
An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im- provement in the performance of a language processing task like speech recognition or machine translation. Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm. But a model’s improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model.
3.3 Generalization and Zeros
The n-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a given training corpus. Another implication is that n-grams do a better and better job of modeling the training corpus as we increase the value of N.
3.3 • GENERALIZATION AND ZEROS 9
PP(W) = P(w w ...w )− 12N
1
N 1N1
=( )− 10
= 1−1 10
= 10
N
(3.17)
   Unigram Bigram Trigram
 Perplexity
 962 170 109

10 CHAPTER 3 • N-GRAM LANGUAGE MODELS
We can visualize both of these facts by borrowing the technique of Shannon (1951) and Miller and Selfridge (1950) of generating random sentences from dif- ferent n-gram models. It’s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency. We choose a random value between 0 and 1 and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>. We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability). Let’s say the second word of that bigram is w. We next chose a random bigram starting with w (again, drawn according to its bigram probability), and so on.
To give an intuition for the increasing power of higher-order n-grams, Fig. 3.3 shows random sentences generated from unigram, bigram, trigram, and 4-gram models trained on Shakespeare’s works.
  1
–To him swallowed confess hear both. Which. Of save on trail for are ay device and
rote life have
gram –Hill he late speaks; or! a more to leg less first you enter
–Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live
2
king. Follow.
gram –What means, sir. I confess she? then all sorts, he is trim, captain.
–Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,
3
’tis done.
gram –This shall forbid it should be branded, if renown made it empty.
–King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A
4
great banquet serv’d in; gram –It cannot be but so.
  Figure 3.3
Eight sentences randomly generated from four n-grams computed from Shakespeare’s works. All characters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected for capitalization to improve readability.
The longer the context on which we train the model, the more coherent the sen-
tences. In the unigram sentences, there is no coherent relation between words or any
sentence-final punctuation. The bigram sentences have some local word-to-word
coherence (especially if we consider that punctuation counts as a word). The tri-
gram and 4-gram sentences are beginning to look a lot like Shakespeare. Indeed, a
careful investigation of the 4-gram sentences shows that they look a little too much
like Shakespeare. The words It cannot be but so are directly from King John. This is
because, not to put the knock on Shakespeare, his oeuvre is not very large as corpora
go (N = 884, 647, V = 29, 066), and our n-gram probability matrices are ridiculously
sparse. There are V 2 = 844, 000, 000 possible bigrams alone, and the number of pos- 4 17
sible 4-grams is V = 7 × 10 . Thus, once the generator has chosen the first 4-gram (It cannot be but), there are only five possible continuations (that, I, he, thou, and so); indeed, for many 4-grams, there is only one continuation.
To get an idea of the dependence of a grammar on its training set, let’s look at an n-gram grammar trained on a completely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres. Fig. 3.4

3.3 • GENERALIZATION AND ZEROS 11 shows sentences generated by unigram, bigram, and trigram grammars trained on
40 million words from WSJ.
  1 gram
Months the my and issue of year foreign new exchange’s september
were recession exchange new endorsed a acquire to six executives
Last December through the way to preserve the Hudson corporation N.
2
B. E. C. Taylor would seem to complete the major central planners one gram point five percent of U. S. E. has already old M. X. corporation of living
on information such as more frequently fishing to keep her
They also point to ninety nine point six billion dollars from two hundred
3
four oh six three percent of the rates of interest stores as Mexico and gram Brazil on market conditions
  Figure 3.4
Three sentences randomly generated from three n-gram models computed from 40 million words of the Wall Street Journal, lower-casing all characters and treating punctua- tion as words. Output was then hand-corrected for capitalization to improve readability.
Compare these examples to the pseudo-Shakespeare in Fig. 3.3. While they both model “English-like sentences”, there is clearly no overlap in generated sentences, and little overlap even in small phrases. Statistical models are likely to be pretty use- less as predictors if the training sets and the test sets are as different as Shakespeare and WSJ.
How should we deal with this problem when we build n-gram models? One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish. To build a language model for translating legal documents, we need a training corpus of legal documents. To build a language model for a question-answering system, we need a training corpus of questions.
It is equally important to get training data in the appropriate dialect, especially when processing social media posts or spoken transcripts. Thus tweets in AAVE (African American Vernacular English) often use words like finna—an auxiliary verb that marks immediate future tense —that don’t occur in other dialects, or spellings like den for then, in tweets like this one (Blodgett and O’Connor, 2017):
(3.18) Bored af den my phone finna die!!!
while tweets from varieties like Nigerian English have markedly different vocabu-
lary and n-gram patterns from American English (Jurgens et al., 2017):
(3.19) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u
tweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter
Matching genres and dialects is still not sufficient. Our models may still be subject to the problem of sparsity. For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability. But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it. That is, we’ll have many cases of putative “zero probability n-grams” that should really have some non-zero probability. Consider the words that follow the bigram denied the in the WSJ Treebank3 corpus, together with their counts:
denied the allegations: 5 deniedthespeculation: 2 denied the rumors: 1 denied the report: 1
But suppose our test set has phrases like:

12
CHAPTER 3 • N-GRAM LANGUAGE MODELS
denied the offer
closed vocabulary
OOV open vocabulary
zeros
denied the loan
Our model will incorrectly estimate that the P(offer|denied the) is 0!
These zeros— things that don’t ever occur in the training set but do occur in the test set—are a problem for two reasons. First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.
Second, if the probability of any word in the test set is 0, the entire probability of the test set is 0. By definition, perplexity is based on the inverse probability of the test set. Thus if some words have zero probability, we can’t compute perplexity at all, since we can’t divide by 0!
3.3.1 Unknown Words
The previous section discussed the problem of words whose bigram probability is zero. But what about words we simply have never seen before?
Sometimes we have a language task in which this can’t happen because we know all the words that can occur. In such a closed vocabulary system the test set can only contain words from this lexicon, and there will be no unknown words. This is a reasonable assumption in some domains, such as speech recognition or machine translation, where we have a pronunciation dictionary or a phrase table that are fixed in advance, and so the language model can only use the words in that dictionary or phrase table.
In other cases we have to deal with words we haven’t seen before, which we’ll call unknown words, or out of vocabulary (OOV) words. The percentage of OOV words that appear in the test set is called the OOV rate. An open vocabulary system is one in which we model these potential unknown words in the test set by adding a pseudo-word called <UNK>.
There are two common ways to train the probabilities of the unknown word model <UNK>. The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance:
1. Choose a vocabulary (word list) that is fixed in advance.
2. Convert in the training set any word that is not in this set (any OOV word) to
the unknown word token <UNK> in a text normalization step.
3. Estimatetheprobabilitiesfor<UNK>fromitscountsjustlikeanyotherregular
word in the training set.
The second alternative, in situations where we don’t have a prior vocabulary in ad- vance, is to create such a vocabulary implicitly, replacing words in the training data by <UNK> based on their frequency. For example we can replace by <UNK> all words that occur fewer than n times in the training set, where n is some small number, or equivalently select a vocabulary size V in advance (say 50,000) and choose the top V words by frequency and replace the rest by UNK. In either case we then proceed to train the language model as before, treating <UNK> like a regular word.
The exact choice of <UNK> model does have an effect on metrics like perplexity. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. For this reason, perplexities should only be compared across language models with the same vocabularies (Buck et al., 2014).

3.4 Smoothing
smoothing discounting
Laplace smoothing
What do we do with words that are in our vocabulary (they are not unknown words) but appear in a test set in an unseen context (for example they appear after a word they never appeared after in training)? To keep a language model from assigning zero probability to these unseen events, we’ll have to shave off a bit of probability mass from some more frequent events and give it to the events we’ve never seen. This modification is called smoothing or discounting. In this section and the fol- lowing ones we’ll introduce a variety of ways to do smoothing: add-1 smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing.
3.4.1 Laplace Smoothing
The simplest way to do smoothing is to add one to all the bigram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace smoothing. Laplace smoothing does not perform well enough to be used in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4).
Let’s start with the application of Laplace smoothing to unigram probabilities. Recall that the unsmoothed maximum likelihood estimate of the unigram probability of the word wi is its count ci normalized by the total number of word tokens N:
P(wi) = ci N
Laplace smoothing merely adds one to each count (hence its alternate name add- one smoothing). Since there are V words in the vocabulary and each one was incre- mented, we also need to adjust the denominator to take into account the extra V observations. (What happens to our P values if we don’t increase the denominator?)
PLaplace(wi) = ci + 1 (3.20) N+V
Instead of changing both the numerator and denominator, it is convenient to describe how a smoothing algorithm affects the numerator, by defining an adjusted
∗
add-one
discounting
discount
count c . This adjusted count is easier to compare directly with the MLE counts and can be turned into a probability like an MLE count by normalizing by N. To define this count, since we are only changing the numerator in addition to adding 1 we’ll also need to multiply by a normalization factor N :
(3.21)
A related way to view smoothing is as discounting (lowering) some non-zero counts in order to get the probability mass that will be assigned to the zero counts. Thus, instead of referring to the discounted counts c∗, we might describe a smooth- ing algorithm in terms of a relative discount dc, the ratio of the discounted counts to the original counts:
N+V
We can now turn c∗ into a probability P∗ by normalizing by N.
ii
N+V c∗i =(ci+1) N
3.4 • SMOOTHING 13

14 CHAPTER 3 • N-GRAM LANGUAGE MODELS
dc = c∗ c
Now that we have the intuition for the unigram case, let’s smooth our Berkeley Restaurant Project bigrams. Figure 3.5 shows the add-one smoothed counts for the bigrams in Fig. 3.1.
 i want to eat chinese food lunch spend
 i 6 828 1 10 1 1 1 3 want 3 1 609 2 7 7 6 2 to 3 1 5 687 3 1 7 212 eat 1 1 3 1 17 3 43 1 chinese 2 1 1 1 1 83 2 1 food 16 1 16 1 2 5 1 1 lunch 3 1 1 1 1 2 1 1 spend 2 1 2 1 1 1 1 1
  Figure 3.5
 Add-one smoothed bigram counts for eight of the words (out of V = 1446) in the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.
Figure 3.6 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2. Recall that normal bigram probabilities are computed by normalizing each row of counts by the unigram count:
P(wn|wn−1) = C(wn−1wn) (3.22) C(wn−1)
For add-one smoothed bigram counts, we need to augment the unigram count by the number of total word types in the vocabulary V :
 ∗
C(wn−1wn) + 1 (wn|wn−1) = 􏰁 =
C(wn−1wn) + 1 C(wn−1)+V
(3.23) Thus, each of the unigram counts given in the previous section will need to be
P
  Laplace
w (C(wn−1w)+1)
augmented by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.6.
 i 0.0015 want 0.0013 to 0.00078 eat 0.00046 chinese 0.0012 food 0.0063 lunch 0.0017 spend 0.0012
0.21 0.00025 0.0025 0.00042 0.26 0.00084 0.00026 0.0013 0.18 0.00046 0.0014 0.00046 0.00062 0.00062 0.00062 0.00039 0.0063 0.00039 0.00056 0.00056 0.00056 0.00058 0.0012 0.00058
0.00025 0.00025 0.00025
0.0029 0.0029 0.0025 0.00078 0.00026 0.0018 0.0078 0.0014 0.02 0.00062 0.052 0.0012 0.00079 0.002 0.00039 0.00056 0.0011 0.00056 0.00058 0.00058 0.00058
0.00075 0.00084 0.055 0.00046 0.00062 0.00039 0.00056 0.00058
i want to eat chinese food lunch spend
   Figure 3.6
 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP corpus of 9332 sentences. Previously-zero probabilities are in gray.
It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts. These adjusted counts can be computed by Eq. 3.24. Figure 3.7 shows the reconstructed counts.
c∗(wn−1wn) = [C(wn−1wn)+1]×C(wn−1) (3.24) C(wn−1)+V

add-k
Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus of 9332 sentences. Previously-zero counts are in gray.
Note that add-one smoothing has made a very big change to the counts. C(want to) changed from 609 to 238! We can see this in probability space as well: P(to|want) decreases from .66 in the unsmoothed case to .26 in the smoothed case. Looking at the discount d (the ratio between new and old counts) shows us how strikingly the counts for each prefix word have been reduced; the discount for the bigram want to is .39, while the discount for Chinese food is .10, a factor of 10!
The sharp change in counts and probabilities occurs because too much probabil- ity mass is moved to all the zeros.
3.4.2 Add-k smoothing
One alternative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events. Instead of adding 1 to each count, we add a frac- tional count k (.5? .05? .01?). This algorithm is therefore called add-k smoothing.
P∗ (wn|wn−1) = C(wn−1wn) + k (3.25) Add-k C(wn−1)+kV
Add-k smoothing requires that we have a method for choosing k; this can be done, for example, by optimizing on a devset. Although add-k is useful for some tasks (including text classification), it turns out that it still doesn’t work well for language modeling, generating counts with poor variances and often inappropriate discounts (Gale and Church, 1994).
3.4.3 Backoff and Interpolation
The discounting we have been discussing so far can help solve the problem of zero frequency n-grams. But there is an additional source of knowledge we can draw on. If we are trying to compute P(wn|wn−2wn−1) but we have no examples of a particular trigram wn−2wn−1wn, we can instead estimate its probability by using the bigram probability P(wn|wn−1). Similarly, if we don’t have counts to compute P(wn|wn−1), we can look to the unigram P(wn).
In other words, sometimes using less context is a good thing, helping to general- ize more for contexts that the model hasn’t learned much about. There are two ways to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram. In other words, we only “back off” to a lower-order n-gram if we have zero evidence for a higher-order n-gram. By contrast, in interpolation, we always mix the probability estimates from all the n-gram estimators, weighing and combining the trigram, bigram, and unigram counts.
3.4 • SMOOTHING 15
  i want to eat chinese food lunch spend
 i 3.8 527 want 1.2 0.39 to 1.9 0.63 eat 0.34 0.34 chinese 0.2 0.098 food 6.9 0.43 lunch 0.57 0.19 spend 0.32 0.16
0.64 6.4 0.64 238 0.78 2.7 3.1 430 1.9
1 0.34 5.8 0.098 0.098 0.098 6.9 0.43 0.86 0.19 0.19 0.19 0.32 0.16 0.16
0.64 0.64 1.9 2.7 2.3 0.78 0.63 4.4 133 1 15 0.34 8.2 0.2 0.098 2.2 0.43 0.43 0.38 0.19 0.19 0.16 0.16 0.16
  Figure 3.7
 backoff
interpolation

16
CHAPTER 3 • N-GRAM LANGUAGE MODELS
held-out
How are these λ values set? Both the simple interpolation and conditional inter- polation λ s are learned from a held-out corpus. A held-out corpus is an additional training corpus that we use to set hyperparameters like these λ values, by choosing the λ values that maximize the likelihood of the held-out corpus. That is, we fix the n-gram probabilities and then search for the λ values that—when plugged into Eq. 3.26—give us the highest probability of the held-out set. There are various ways to find this optimal set of λs. One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal λ s (Jelinek and Mercer, 1980).
In a backoff n-gram model, if the n-gram we need has zero counts, we approxi- mate it by backing off to the (N-1)-gram. We continue backing off until we reach a history that has some counts.
In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t discounted and we just used the undiscounted MLE probability, then as soon as we replaced an n-gram which has zero probability with a lower-order n-gram, we would be adding probability mass, and the total probability assigned to all possible strings by the language model would be greater than 1! In addition to this explicit discount factor, we’ll need a function α to distribute this probability mass to the lower order n-grams.
This kind of backoff with discounting is also called Katz backoff. In Katz back- off we rely on a discounted probability P∗ if we’ve seen this n-gram before (i.e., if we have non-zero counts). Otherwise, we recursively back off to the Katz probabil- ity for the shorter-history (N-1)-gram. The probability for a backoff n-gram PBO is
discount
Katz backoff
In simple linear interpolation, we combine different order n-grams by linearly in- terpolating all the models. Thus, we estimate the trigram probability P(wn|wn−2wn−1) by mixing together the unigram, bigram, and trigram probabilities, each weighted by a λ:
such that the λs sum to 1:
+λ2 P(wn |wn−1 ) +λ3 P(wn )
􏰂λi =1 i
(3.26)
(3.27)
ˆ
P(wn|wn−2wn−1) = λ1P(wn|wn−2wn−1)
In a slightly more sophisticated version of linear interpolation, each λ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the λs for those trigrams higher and thus give that trigram more weight in the interpolation. Equation 3.28 shows the equation for interpolation with context-conditioned weights:
Pˆ(wn|wn−2wn−1) = λ1(wn−1)P(wn|wn−2wn−1) n−2
+λ2 (wn−1 )P(wn |wn−1 ) n−2
+ λ3 (wn−1 )P(wn ) n−2
(3.28)

Good-Turing
(3.29) Katz backoff is often combined with a smoothing method called Good-Turing. The combined Good-Turing backoff algorithm involves quite detailed computation
thus computed as follows:
P (w |wn−1 )= BO n n−N+1
),
 α(wn−N+1)PBO(wn|wn−N+2),
 P∗(wn|wn−1 n−N+1
if C(wn n−N+1
otherwise.
) > 0
for estimating the Good-Turing smoothing and the P∗ and α values. 3.5 Kneser-Ney Smoothing
Kneser-Ney
One of the most commonly used and best performing n-gram smoothing methods is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Good- man 1998).
Kneser-Ney has its roots in a method called absolute discounting. Recall that discounting of the counts for frequent n-grams is necessary to save some probability mass for the smoothing algorithm to distribute to the unseen n-grams.
To see this, we can use a clever idea from Church and Gale (1991). Consider an n-gram that has count 4. We need to discount this count by some amount. But how much should we discount it? Church and Gale’s clever idea was to look at a held-out corpus and just see what the count is for all those bigrams that had count 4 in the training set. They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words. On average, a bigram that occurred 4 times in the first 22 million words occurred 3.23 times in the next 22 million words. The following table from Church and Gale (1991) shows these counts for bigrams with c from 0 to 9:
n−1
n−1
3.5
•
KNESER-NEY SMOOTHING 17
  Bigramcountin Bigramcountin
 trainingset heldoutset
  0 0.0000270
 1 0.448
 2 1.25
 3 2.24
 4 3.23
 5 4.21
 6 5.23
 7 6.21
 8 7.21
 9 8.26
  Absolute discounting
Figure 3.8 For all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the counts of these bigrams in a held-out corpus also of 22 million words.
The astute reader may have noticed that except for the held-out counts for 0 and 1, all the other bigram counts in the held-out set could be estimated pretty well by just subtracting 0.75 from the count in the training set! Absolute discounting formalizes this intuition by subtracting a fixed (absolute) discount d from each count. The intuition is that since we have good estimates already for the very high counts, a small discount d won’t affect them much. It will mainly modify the smaller counts,

18 CHAPTER 3 • N-GRAM LANGUAGE MODELS
for which we don’t necessarily trust the estimate anyway, and Fig. 3.8 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9. The equation for interpolated absolute discounting applied to bigrams:
C(wi−1wi) − d
PAbsoluteDiscounting(wi|wi−1) = 􏰁vC(wi−1 v) +λ(wi−1)P(wi) (3.30)
The first term is the discounted bigram, and the second term is the unigram with an interpolation weight λ . We could just set all the d values to .75, or we could keep a separate discount value of 0.5 for the bigrams with counts of 1.
Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discount- ing with a more sophisticated way to handle the lower-order unigram distribution. Consider the job of predicting the next word in this sentence, assuming we are inter- polating a bigram and a unigram model.
I can’t see without my reading   .
The word glasses seems much more likely to follow here than, say, the word Kong, so we’d like our unigram model to prefer glasses. But in fact it’s Kong that is more common, since Hong Kong is a very frequent word. A standard unigram model will assign Kong a higher probability than glasses. We would like to capture the intuition that although Kong is frequent, it is mainly only frequent in the phrase Hong Kong, that is, after the word Hong. The word glasses has a much wider distribution.
In other words, instead of P(w), which answers the question “How likely is w?”, we’d like to create a unigram model that we might call PCONTINUATION, which answers the question “How likely is w to appear as a novel continuation?”. How can we estimate this probability of seeing the word w as a novel continuation, in a new unseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION on the number of different contexts word w has appeared in, that is, the number of bigram types it completes. Every bigram type was a novel continuation the first time it was seen. We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well. The number of times a word w appears as a novel continuation can be expressed as:
PCONTINUATION(w) ∝ |{v : C(vw) > 0}| (3.31) To turn this count into a probability, we normalize by the total number of word
 bigram types. In summary:
PCONTINUATION(w) = |{v : C(vw) > 0}| (3.32)
 |{(u′,w′):C(u′w′)>0}|
An equivalent formulation based on a different metaphor is to use the number of
word types seen to precede w (Eq. 3.31 repeated):
PCONTINUATION(w) ∝ |{v : C(vw) > 0}| (3.33)
normalized by the number of words preceding all words, as follows: |{v : C(vw) > 0}|
PCONTINUATION(w) = 􏰁w′ |{v : C(vw′) > 0}| (3.34)
 A frequent word (Kong) occurring in only one context (Hong) will have a low continuation probability.

Interpolated Kneser-Ney
3.6 • THE WEB AND STUPID BACKOFF 19 The final equation for Interpolated Kneser-Ney smoothing for bigrams is then:
PKN(wi|wi−1) = max(C(wi−1wi)−d,0) +λ(wi−1)PCONTINUATION(wi) (3.35) C(wi−1)
The λ is a normalizing constant that is used to distribute the probability mass we’ve discounted.:
d
λ(wi−1)= 􏰁vC(wi−1v)|{w:C(wi−1w)>0}| (3.36)
d
The first term, 􏰁vC(wi−1v), is the normalized discount. The second term,
|{w : C(wi−1w) > 0}|, is the number of word types that can follow wi−1 or, equiva- lently, the number of word types that we discounted; in other words, the number of times we applied the normalized discount.
   The general recursive formulation is as follows:
i−1 max(cKN(wi−n+1)−d,0) i−1 i−1
PKN(wi|wi−n+1)= 􏰁 cKN(wi−1 v) +λ(wi−n+1)PKN(wi|wi−n+2) (3.37) v i−n+1
 modified Kneser-Ney
cabulary entry with count zero, and hence its probability will be a lambda-weighted
λ(ε) uniform distribution V .
The best-performing version of Kneser-Ney smoothing is called modified Kneser- Ney smoothing, and is due to Chen and Goodman (1998). Rather than use a single fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and d3+ for n-grams with counts of 1, 2 and three or more, respectively. See Chen and Goodman (1998, p. 19) or Heafield et al. (2013) for the details.
where the definition of the count cKN depends on whether we are counting the highest-order n-gram being interpolated (for example trigram if we are interpolating trigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram if we are interpolating trigram, bigram, and unigram):
􏰀 count(·) for the highest order
cKN (·) = continuationcount(·) for lower orders (3.38)
The continuation count is the number of unique single word contexts for ·.
At the termination of the recursion, unigrams are interpolated with the uniform
distribution, where the parameter ε is the empty string: max(cKN(w)−d,0) 1
PKN(w) = 􏰁w′ cKN(w′) +λ(ε)V (3.39) If we want to include an unknown word <UNK>, it’s just included as a regular vo-
 3.6 The Web and Stupid Backoff
By using text from the web, it is possible to build extremely large language mod- els. In 2006 Google released a very large set of n-gram counts, including n-grams (1-grams through 5-grams) from all the five-word sequences that appear at least

20
CHAPTER 3 • N-GRAM LANGUAGE MODELS
Bloom filters
Efficiency considerations are important when building language models that use such large sets of n-grams. Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.
N-grams can also be shrunk by pruning, for example only storing n-grams with counts greater than some threshold (such as the count threshold of 40 used for the Google n-gram release) or using entropy to prune less-important n-grams (Stolcke, 1998). Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al. 2007). Finally, effi- cient language model toolkits like KenLM (Heafield 2011, Heafield et al. 2013) use sorted arrays, efficiently combine probabilities and backoffs in a single value, and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large corpus.
Although with these toolkits it is possible to build web-scale language models using full Kneser-Ney smoothing, Brants et al. (2007) show that with very large lan- guage models a much simpler algorithm may be sufficient. The algorithm is called stupid backoff. Stupid backoff gives up the idea of trying to make the language model a true probability distribution. There is no discounting of the higher-order probabilities. If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed (context-independent) weight. This algo- rithm does not produce a probability distribution, so we’ll follow Brants et al. (2007) in referring to it as S:
stupid backoff
40 times from 1,024,908,267,229 words of running text on the web; this includes 1,176,470,663 five-word sequences using over 13 million unique words types (Franz and Brants, 2006). Some examples:
4-gram   Count
  serve as the incoming 92
 serve as the incubator 99
 serve as the independent 794
 serve as the index 223
 serve as the indication 72
 serve as the indicator 120
 serve as the indicators 45
 serve as the indispensable 111
 serve as the indispensible 40
 serve as the individual 234
 S(w |wi−1
i i−k+1
 count(wi )  i−k+1
if count(wi i−k+1
) > 0
count(wi−1 ) i−k+1
) =
The backoff terminates in the unigram, which has probability S(w) = N
(3.40) . Brants
 λS(wi|wi−1 i−k+2
)
otherwise et al. (2007) find that a value of 0.4 worked well for λ .
count (w)
3.7 Advanced: Perplexity’s Relation to Entropy
We introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on a test set. A better n-gram model is one that assigns a higher probability to the

Entropy
3.7 • ADVANCED: PERPLEXITY’S RELATION TO ENTROPY 21
test data, and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example?) and its relationship to entropy. Entropy is a measure of information. Given a random variable X ranging over whatever we are predicting (words, letters, parts of speech, the set of which we’ll call χ) and with a particular probability function, call it p(x), the entropy of the random variable X is:
􏰂
H(X) = −
p(x)log2 p(x) (3.41)
x∈χ
The log can, in principle, be computed in any base. If we use log base 2, the
resulting value of entropy will be measured in bits.
One intuitive way to think about entropy is as a lower bound on the number of
bits it would take to encode a certain decision or piece of information in the optimal coding scheme.
Consider an example from the standard information theory textbook Cover and Thomas (1991). Imagine that we want to place a bet on a horse race but it is too far to go all the way to Yonkers Racetrack, so we’d like to send a short message to the bookie to tell him which of the eight horses to bet on. One way to encode this message is just to use the binary representation of the horse’s number as the code; thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded as 000. If we spend the whole day betting and each horse is coded with 3 bits, on average we would be sending 3 bits per race.
Can we do better? Suppose that the spread is the actual distribution of the bets placed and that we represent it as the prior probability of each horse as follows:
1
64 1
64 1
64 1
64
The entropy of the random variable X that ranges over horses gives us a lower bound on the number of bits and is
  Horse 1 Horse 2 Horse 3
1
2 1
4 1
8 1
16
 Horse 5 Horse 6 Horse 7
 Horse 4
 Horse 8
 = 2bits
(3.42)
i=8 􏰂
H(X) = −
= −1log1−1log1−1log1−1 log 1 −4(1 log 1 )
p(i)logp(i)
2 2 4 4 8 8 16 16 64 64
i=1
A code that averages 2 bits per race can be built with short encodings for more probable horses, and longer encodings for less probable horses. For example, we could encode the most likely horse with the code 0, and the remaining horses as 10, then 110, 1110, 111100, 111101, 111110, and 111111.
What if the horses are equally likely? We saw above that if we used an equal-
length binary code for the horse numbers, each horse took 3 bits to code, so the
average was 3. Is the entropy the same? In this case each horse would have a
1
probability of 8 . The entropy of the choice of horses is then
i=811 1
H(X)=− log =−log =3bits (3.43)
i=188 8
􏰂

22
CHAPTER 3 • N-GRAM LANGUAGE MODELS
entropy rate
W 1n ∈ L
entropy) as the entropy of this sequence divided by the number of words:
Stationary
n
But to measure the true entropy of a language, we need to consider sequences of infinite length. If we think of a language as a stochastic process L that produces a sequence of words, and allow W to represent the sequence of words w1 , . . . , wn , then L’s entropy rate H(L) is defined as
H(L) = lim 1H(w1,w2,...,wn) n→∞ n
= − lim 1 􏰂 p(w1,...,wn)log p(w1,...,wn) (3.46) n→∞ n W ∈L
The Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and Thomas 1991) states that if the language is regular in certain ways (to be exact, if it is both stationary and ergodic),
1
H(L) = lim − log p(w1w2 ...wn) (3.47)
n→∞ n
That is, we can take a single sequence that is long enough instead of summing over all possible sequences. The intuition of the Shannon-McMillan-Breiman the- orem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer se- quence according to their probabilities.
A stochastic process is said to be stationary if the probabilities it assigns to a sequence are invariant with respect to shifts in the time index. In other words, the probability distribution for words at time t is the same as the probability distribution at time t + 1. Markov models, and hence n-grams, are stationary. For example, in a bigram, Pi is dependent only on Pi−1. So if we shift our time index by x, Pi+x is still dependent on Pi+x−1. But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent. Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.
To summarize, by making some incorrect but convenient simplifying assump- tions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.
Now we are ready to introduce cross-entropy. The cross-entropy is useful when we don’t know the actual probability distribution p that generated some data. It
cross-entropy
Until now we have been computing the entropy of a single variable. But most of what we will use entropy for involves sequences. For a grammar, for example, we willbecomputingtheentropyofsomesequenceofwordsW ={w0,w1,w2,...,wn}. One way to do this is to have a variable that ranges over sequences of words. For example we can compute the entropy of a random variable that ranges over all finite sequences of words of length n in some language L as follows:
1n1􏰂nn
􏰂nn
p(W1 )log p(W1 ) (3.44)
H(w1,w2,...,wn) = −
We could define the entropy rate (we could also think of this as the per-word
H(W1 ) = − p(W1 )log p(W1 ) (3.45) n W 1n ∈ L

perplexity
3.7 • ADVANCED: PERPLEXITY’S RELATION TO ENTROPY 23 allows us to use some m, which is a model of p (i.e., an approximation to p). The
cross-entropy of m on p is defined by
1􏰂
H(p,m) = lim − p(w1,...,wn)logm(w1,...,wn) (3.48)
n→∞ nW∈L
That is, we draw sequences according to the probability distribution p, but sum the log of their probabilities according to m.
Again, following the Shannon-McMillan-Breiman theorem, for a stationary er- godic process:
1
H(p,m) = lim − logm(w1w2 ...wn) (3.49)
n→∞ n
This means that, as for entropy, we can estimate the cross-entropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences.
What makes the cross-entropy useful is that the cross-entropy H(p,m) is an up- per bound on the entropy H(p). For any model m:
H(p) ≤ H(p,m) (3.50)
This means that we can use some simplified model m to help estimate the true en- tropy of a sequence of symbols drawn according to probability p. The more accurate m is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus, the difference between H(p,m) and H(p) is a measure of how accurate a model is. Between two models m1 and m2, the more accurate model will be the one with the lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy.)
We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq. 3.49. Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity. We will need an approximation to cross- entropy, relying on a (sufficiently long) sequence of fixed length. This approxima- tion to the cross-entropy of a model M = P(wi|wi−N+1...wi−1) on a sequence of words W is
1
H(W) = −
The perplexity of a model P on a sequence of words W is now formally defined as
the exp of this cross-entropy:
N
logP(w1w2 ...wN) (3.51)
Perplexity(W) = 2H(W)
= P(w w ...w )−
1
N
12N 􏰄
 =N1 P(w1w2 ...wN)
􏰇􏰆􏰆􏰅N 􏰃N 1
P(wi|w1 ...wi−1)
  =
(3.52)
 i=1

24 CHAPTER 3 • N-GRAM LANGUAGE MODELS 3.8 Summary
This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.
• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.
• n-grams are Markov models that estimate words from a fixed window of pre- vious words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).
• n-gram language models are evaluated extrinsically in some task, or intrinsi- cally using perplexity.
• The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.
• Smoothingalgorithmsprovideamoresophisticatedwaytoestimatetheprob- ability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.
• Bothbackoffandinterpolationrequirediscountingtocreateaprobabilitydis- tribution.
• Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.
Bibliographical and Historical Notes
The underlying mathematics of the n-gram was first proposed by Markov (1913), who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a con- sonant. Markov classified 20,000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters. Shannon (1948) applied n-grams to compute approximations to English word sequences. Based on Shannon’s work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s. In a series of extremely influential papers starting with Chomsky (1956) and including Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued that “finite-state Markov processes”, while a possibly useful engineering heuristic, were incapable of being a complete cognitive model of human grammatical knowl- edge. These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.
The resurgence of n-gram models came from Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and Baker
at CMU, who was influenced by the work of Baum and colleagues. Independently
these two labs successfully used n-grams in their speech recognition systems (Baker 1975b, Jelinek 1976, Baker 1975a, Bahl et al. 1983, Jelinek 1990). A trigram model was
used in the IBM TANGORA speech recognition system in the 1970s, but the idea
was not written up until later.
Add-one smoothing derives from Laplace’s 1812 law of succession and was first applied as an engineering solution to the zero-frequency problem by Jeffreys (1948)

class-based n-gram
EXERCISES 25
based on an earlier Add-K suggestion by Johnson (1932). Problems with the add- one algorithm are summarized in Gale and Church (1994).
A wide variety of different language modeling and smoothing techniques were proposed in the 80s and 90s, including Good-Turing discounting—first applied to the n-gram smoothing at IBM by Katz (Na ́das 1984, Church and Gale 1991)— Witten-Bell discounting (Witten and Bell, 1991), and varieties of class-based n- gram models that used information about word classes.
Starting in the late 1990s, Chen and Goodman produced a highly influential series of papers with a comparison of different language models (Chen and Good- man 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006). They performed a number of carefully controlled experiments comparing differ- ent discounting algorithms, cache models, class-based models, and other language model parameters. They showed the advantages of Modified Interpolated Kneser- Ney, which has since become the standard baseline for language modeling, espe- cially because they showed that caches and class-based models provided only minor additional improvement. These papers are recommended for any reader with further interest in language modeling.
Two commonly used toolkits for building language models are SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al. 2013). Both are publicly available. SRILM offers a wider range of options and types of discounting, while KenLM is optimized for speed and memory size, making it possible to build web-scale lan- guage models.
The highest accuracy language models are neural network language models. These solve a major problem with n-gram language models: the number of parame- ters increases exponentially as the n-gram order increases, and n-grams have no way to generalize from training to test set. Neural language models instead project words into a continuous space in which words with similar contexts have similar represen- tations. We’ll introduce both feedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7, and recurrent language models (Mikolov, 2012) in Chapter 9.
Exercises
3.1 Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 4.
3.2 Calculate the probability of the sentence i want chinese food. Give two probabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on page 6, and another using the add-1 smoothed table in Fig. 3.6. Assume the additional add-1 smoothed probabilities P(i|<s>) = 0.19 and P(</s>|food) = 0.40.
3.3 Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why.
3.4 We are given the following corpus, modified from the one in the chapter:
                 <s> I am Sam </s>
                 <s> Sam I am </s>
                 <s> I am Sam </s>
                 <s> I do not like green eggs and Sam </s>

26 CHAPTER 3 •
3.5
N-GRAM LANGUAGE MODELS
Using a bigram language model with add-one smoothing, what is P(Sam |
am)? Include <s> and </s> in your counts just like any other token.
Suppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram
grammar on the following training corpus without using the end-symbol </s>:
  <s> a b
  <s> b b
  <s> b a
  <s> a a
Demonstrate that your bigram model does not assign a single probability dis- tribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet {a,b} is also 1.0.
Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for esti- mating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2), in terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and so on for bigrams and unigrams.
We are given the following corpus, modified from the one in the chapter:
<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>
If we use linear interpolation smoothing between a maximum-likelihood bi- gram model and a maximum-likelihood unigram model with λ1 = 21 and λ2 = 12 , what is P(Sam|am)? Include <s> and </s> in your counts just like any
other token.
Write a program to compute unsmoothed unigrams and bigrams.
Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?
Add an option to your program to generate random sentences.
Add an option to your program to compute the perplexity of a test set.
Given a training set of 100 numbers consists of 91 zeros and 1 each of the otherdigits1-9. Nowweseethefollowingtestset: 0000030000. What is the unigram perplexity?
3.6
3.7
3.8 3.9
3.10 3.11 3.12

 Algoet, P. H. and Cover, T. M. (1988). A sandwich proof of the Shannon-McMillan-Breiman theorem. The Annals of Probability, 16(2), 899–909.
Bahl, L. R., Jelinek, F., and Mercer, R. L. (1983). A max- imum likelihood approach to continuous speech recogni- tion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2), 179–190.
Baker, J. K. (1975a). The DRAGON system – An overview.
IEEE Transactions on Acoustics, Speech, and Signal Pro- cessing, ASSP-23(1), 24–29.
Baker, J. K. (1975b). Stochastic modeling for automatic speech understanding. In Reddy, D. R. (Ed.), Speech Recognition. Academic Press.
Bengio, Y., Schwenk, H., Sene ́cal, J.-S., Morin, F., and Gau- vain, J.-L. (2006). Neural probabilistic language models. In Innovations in Machine Learning, 137–186. Springer.
Blodgett, S. L. and O’Connor, B. (2017). Racial disparity in natural language processing: A case study of social media african-american english. In Fairness, Accountability, and Transparency in Machine Learning (FAT/ML) Workshop, KDD.
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. (2007). Large language models in machine translation. In EMNLP/CoNLL 2007.
Buck, C., Heafield, K., and Van Ooyen, B. (2014). N-gram counts and language models from the common crawl. In Proceedings of LREC.
Chen, S. F. and Goodman, J. (1996). An empirical study of smoothing techniques for language modeling. In ACL-96, 310–318.
Chen, S. F. and Goodman, J. (1998). An empirical study of smoothing techniques for language modeling. Tech. rep. TR-10-98, Computer Science Group, Harvard University.
Chen, S. F. and Goodman, J. (1999). An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13, 359–394.
Chomsky, N. (1956). Three models for the description of language. IRE Transactions on Information Theory, 2(3), 113–124.
Chomsky, N. (1957). Syntactic Structures. Mouton, The Hague.
Church, K. W. and Gale, W. A. (1991). A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5, 19–54.
Church, K. W., Hart, T., and Gao, J. (2007). Compress- ing trigram language models with Golomb coding. In EMNLP/CoNLL 2007, 199–207.
Cover, T. M. and Thomas, J. A. (1991). Elements of Infor- mation Theory. Wiley.
Franz, A. and Brants, T. (2006). All our n-gram are belong to you. http://googleresearch.blogspot.com/2006/ 08/all- our- n- gram- are- belong- to- you.html.
Gale, W. A. and Church, K. W. (1994). What is wrong with adding one?. In Oostdijk, N. and de Haan, P. (Eds.), Corpus-Based Research into Language, 189–198. Rodopi.
Goodman, J. (2006). A bit of progress in language mod- eling: Extended version. Tech. rep. MSR-TR-2001-72, Machine Learning and Applied Statistics Group, Microsoft Research, Redmond, WA.
Exercises 27
Heafield, K. (2011). KenLM: Faster and smaller language model queries. In Workshop on Statistical Machine Trans- lation, 187–197.
Heafield, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P. (2013). Scalable modified Kneser-Ney language model es- timation.. In ACL 2013, 690–696.
Jeffreys, H. (1948). Theory of Probability (2nd Ed.). Claren- don Press. Section 3.23.
Jelinek, F. (1976). Continuous speech recognition by statis- tical methods. Proceedings of the IEEE, 64(4), 532–557.
Jelinek, F. (1990). Self-organized language modeling for speech recognition. In Waibel, A. and Lee, K.-F. (Eds.), Readings in Speech Recognition, 450–506. Morgan Kauf- mann. Originally distributed as IBM technical report in 1985.
Jelinek, F. and Mercer, R. L. (1980). Interpolated estimation of Markov source parameters from sparse data. In Gelsema, E. S. and Kanal, L. N. (Eds.), Proceedings, Workshop on Pattern Recognition in Practice, 381–397. North Holland.
Johnson, W. E. (1932). Probability: deductive and inductive problems (appendix to). Mind, 41(164), 421–423.
Jurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stolcke, A., Fosler, E., and Morgan, N. (1994). The Berkeley restau- rant project. In ICSLP-94, 2139–2142.
Jurgens, D., Tsvetkov, Y., and Jurafsky, D. (2017). Incorpo- rating dialectal variability for socially equitable language identification. In ACL 2017, 51–57.
Kane, S. K., Morris, M. R., Paradiso, A., and Campbell, J. (2017). “at times avuncular and cantankerous, with the reflexes of a mongoose”: Understanding self-expression through augmentative and alternative communication de- vices. In CSCW 2017, 1166–1179.
Kneser, R. and Ney, H. (1995). Improved backing-off for M- gram language modeling. In ICASSP-95, Vol. 1, 181–184.
Markov, A. A. (1913). Essai d’une recherche statistique sur le texte du roman “Eugene Onegin” illustrant la liaison des epreuve en chain (‘Example of a statistical investigation of the text of “Eugene Onegin” illustrating the dependence be- tween samples in chain’). Izvistia Imperatorskoi Akademii Nauk (Bulletin de l’Acade ́mie Impe ́riale des Sciences de St.-Pe ́tersbourg), 7, 153–162.
Mikolov, T. (2012). Statistical language models based on neural networks. Ph.D. thesis, Ph. D. thesis, Brno Univer- sity of Technology.
Miller, G. A. and Chomsky, N. (1963). Finitary models of language users. In Luce, R. D., Bush, R. R., and Galanter, E. (Eds.), Handbook of Mathematical Psychology, Vol. II, 419–491. John Wiley.
Miller, G. A. and Selfridge, J. A. (1950). Verbal context and the recall of meaningful material. American Journal of Psychology, 63, 176–185.
Na ́das, A. (1984). Estimation of probabilities in the language model of the IBM speech recognition system. IEEE Trans- actions on Acoustics, Speech, Signal Processing, 32(4), 859–861.
Schwenk, H. (2007). Continuous space language models. Computer Speech & Language, 21(3), 492–518.
Shannon, C. E. (1948). A mathematical theory of commu- nication. Bell System Technical Journal, 27(3), 379–423. Continued in the following volume.

28 Chapter 3 • N-gram Language Models Shannon, C. E. (1951). Prediction and entropy of printed
English. Bell System Technical Journal, 30, 50–64.
Stolcke, A. (1998). Entropy-based pruning of backoff lan- guage models. In Proc. DARPA Broadcast News Transcrip- tion and Understanding Workshop, 270–274.
Stolcke, A. (2002). SRILM – an extensible language model- ing toolkit. In ICSLP-02.
Talbot, D. and Osborne, M. (2007). Smoothed Bloom Fil- ter Language Models: Tera-Scale LMs on the Cheap. In EMNLP/CoNLL 2007, 468–476.
Trnka, K., Yarrington, D., McCaw, J., McCoy, K. F., and Pennington, C. (2007). The effects of word prediction on communication rate for AAC. In NAACL-HLT 07, 173– 176.
Witten, I. H. and Bell, T. C. (1991). The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Informa- tion Theory, 37(4), 1085–1094.

Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ⃝c 2019. All rights reserved. Draft of October 2, 2019.
 CHAPTER
4
Naive Bayes and Sentiment Classification
Classification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input. The potential challenges of this task are highlighted by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:
(a) those that belong to the Emperor, (b) embalmed ones, (c) those that are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray dogs, (h) those that are included in this classification, (i) those that tremble as if they were mad, (j) innumerable ones, (k) those drawn with a very fine camel’s hair brush, (l) others, (m) those that have just broken a flower vase, (n) those that resemble flies from a distance.
Many language processing tasks involve classification, although luckily our classes are much easier to define than those of Borges. In this chapter we introduce the naive Bayes algorithm and apply it to text categorization, the task of assigning a label or category to an entire text or document.
We focus on one common text categorization task, sentiment analysis, the ex- traction of sentiment, the positive or negative orientation that a writer expresses toward some object. A review of a movie, book, or product on the web expresses the author’s sentiment toward the product, while an editorial or political text expresses sentiment toward a candidate or political action. Extracting consumer or public sen- timent is thus relevant for fields from marketing to politics.
The simplest version of sentiment analysis is a binary classification task, and the words of the review provide excellent cues. Consider, for example, the follow- ing phrases extracted from positive and negative reviews of movies and restaurants. Words like great, richly, awesome, and pathetic, and awful and ridiculously are very informative cues:
+ ...zany characters and richly applied satire, and some great plot twists − It was pathetic. The worst part about it was the boxing scenes...
+ ...awesome caramel sauce and sweet toasty almonds. I love this place! − ...awful pizza and ridiculously overpriced...
Spam detection is another important commercial application, the binary clas- sification task of assigning an email to one of the two classes spam or not-spam. Many lexical and other features can be used to perform this classification. For ex- ample you might quite reasonably be suspicious of an email containing phrases like “online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”.
Another thing we might want to know about a text is the language it’s written in. Texts on social media, for example, can be in any number of languages and we’ll need to apply different processing. The task of language id is thus the first step in most language processing pipelines. Related tasks like determining a text’s au- thor, (authorship attribution), or author characteristics like gender, age, and native
text categorization
sentiment analysis
spam detection
language id
authorship attribution
2 CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION
supervised machine learning
language are text classification tasks that are also relevant to the digital humanities, social sciences, and forensic linguistics.
Finally, one of the oldest tasks in text classification is assigning a library sub- ject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of infor- mation retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961.
Classification is essential for tasks below the level of the document as well. We’ve already seen period disambiguation (deciding if a period is the end of a sen- tence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 8) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb.
The goal of classification is to take a single observation, extract some useful features, and thereby classify the observation into one of a set of discrete classes. One method for classifying text is to use handwritten rules. There are many areas of language processing where handwritten rule-based classifiers constitute a state-of- the-art system, or at least part of it.
Rules can be fragile, however, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules. Most cases of classification in language processing are instead done via supervised machine learning, and this will be the subject of the remainder of this chapter. In supervised learning, we have a data set of input observations, each associated with some correct output (a ‘supervision signal’). The goal of the algorithm is to learn how to map from a new observation to a correct output.
Formally, the task of supervised classification is to take an input x and a fixed set of output classes Y = y1,y2,...,yM and return a predicted class y ∈ Y. For text classification, we’ll sometimes talk about c (for “class”) instead of y as our output variable, and d (for “document”) instead of x as our input variable. In the supervised situation we have a training set of N documents that have each been hand-labeled with a class: (d1,c1),....,(dN,cN). Our goal is to learn a classifier that is capable of mapping from a new document d to its correct class c ∈ C. A probabilistic classifier additionally will tell us the probability of the observation being in the class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining systems.
Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an ob- servation, they return the class most likely to have generated the observation. Dis- criminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role.

4.1 Naive Bayes Classifiers
naive Bayes classifier
bag-of-words
In this section we introduce the multinomial naive Bayes classifier, so called be- cause it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact.
The intuition of the classifier is shown in Fig. 4.1. We represent a text document as if it were a bag-of-words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. In the example in the figure, instead of representing the word order in all the phrases like “I love this movie” and “I would recommend it”, we simply note that the word I occurred 5 times in the entire excerpt, the word it 6 times, the words love, recommend, and movie once, and so on.
4.1 • NAIVE BAYES CLASSIFIERS 3
  I love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!
   fairy always love to it it whimsical it I
it 6 I5 the 4 to 3 and 3 seen 2 yet 1 would 1 whimsical 1 times 1 sweet 1 satirical 1 adventure 1 genre 1 fairy 1 humor 1 have 1 great 1 ......
and seen are anyone friend dialogue
happy recommend
adventure satirical whosweet of movie
 it it I but to romantic I
several yet humor the again it the
seen would
to scenes I the manages
fun I the timesand
and about whenever have
conventions with
while
   Figure 4.1
Intuition of the multinomial naive Bayes classifier applied to a movie review. The position of the words is ignored (the bag of words assumption) and we make use of the frequency of each word.
Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class cˆ which has the maximum posterior ˆ probability given the document. In Eq. 4.1 we use the hat notation ˆ to mean “our
Bayesian inference
This idea of Bayesian inference has been known since the work of Bayes (1763), and was first applied to text classification by Mosteller and Wallace (1964). The intuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into other probabilities that have some useful properties. Bayes’ rule is presented in Eq. 4.2; it gives us a way to break down any conditional probability P(x|y) into three other probabilities:
P(x|y) = P(y|x)P(x) (4.2) P(y)
estimate of the correct class”.
cˆ = argmaxP(c|d) (4.1) c∈C

4
CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION We can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:
prior probability
likelihood
the same document d, which must have the same probability P(d). Thus, we can choose the class that maximizes this simpler formula:
cˆ = argmaxP(c|d) = argmaxP(d|c)P(c) (4.4) c∈C c∈C
We thus compute the most probable class cˆ given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c):
likelihood prior
􏰇 􏰊􏰉 􏰈 􏰇􏰊􏰉􏰈
cˆ = argmax P(d|c) P(c) (4.5) c∈C
Without loss of generalization, we can represent a document d as a set of features f1, f2,..., fn:
naive Bayes assumption
Unfortunately, Eq. 4.6 is still too hard to compute directly: without some sim- plifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets. Naive Bayes classifiers therefore make two simplifying assumptions.
The first is the bag of words assumption discussed intuitively above: we assume position doesn’t matter, and that the word “love” has the same effect on classification whether it occurs as the 1st, 20th, or last word in the document. Thus we assume that the features f1, f2,..., fn only encode word identity and not position.
The second is commonly called the naive Bayes assumption: this is the condi- tional independence assumption that the probabilities P( fi|c) are independent given the class c and hence can be ‘naively’ multiplied as follows:
cˆ = argmaxP(c|d) = argmax P(d|c)P(c) (4.3) c∈C c∈C P(d)
 We can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This is possible because we will be computing P(d|c)P(c) for each possible class. But P(d)
 P(d)
doesn’t change for each class; we are always asking about the most likely class for
prior
􏰇 􏰊􏰉 􏰈 􏰇􏰊􏰉􏰈
cˆ=argmaxP(f1,f2,....,fn|c) P(c) c∈C
P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)·...·P(fn|c)
The final equation for the class chosen by a naive Bayes classifier is thus:
cNB = argmaxP(c)􏰆P(f|c)
(4.7)
(4.8)
likelihood
c∈C
To apply the naive Bayes classifier to text, we need to consider word positions, by
simply walking an index through every word position in the document: positions ← allwordpositionsintestdocument
cN B = argmax P(c) 􏰆 P(wi |c) (4.9)
c∈C
f∈F
i∈ posit ions
(4.6)

linear classifiers
By considering features in log space, Eq. 4.10 computes the predicted class as a lin- ear function of input features. Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression— are called linear classifiers.
4.2 • TRAINING THE NAIVE BAYES CLASSIFIER 5
Naive Bayes calculations, like calculations for language modeling, are done in log space, to avoid underflow and increase speed. Thus Eq. 4.9 is generally instead expressed as
cN B = argmax log P(c) + 􏰅 log P(wi |c) (4.10)
c∈C
i∈ posit ions
4.2 Training the Naive Bayes Classifier
How can we learn the probabilities P(c) and P(fi|c)? Let’s first consider the max- imum likelihood estimate. We’ll simply use the frequencies in the data. For the document prior P(c) we ask what percentage of the documents in our training set are in each class c. Let Nc be the number of documents in our training data with class c and Ndoc be the total number of documents. Then:
Pˆ(c) = Nc (4.11) Ndoc
To learn the probability P( fi|c), we’ll assume a feature is just the existence of a word in the document’s bag of words, and so we’ll want P(wi|c), which we compute as the fraction of times the word wi appears among all words in all documents of topic c. We first concatenate all documents with category c into one big “category c” text. Then we use the frequency of wi in this concatenated document to give a maximum likelihood estimate of the probability:
ˆ count(wi,c)
P(wi |c) = 􏰄w∈V count (w, c) (4.12)
Here the vocabulary V consists of the union of all the word types in all classes, not just the words in one class c.
There is a problem, however, with maximum likelihood training. Imagine we are trying to estimate the likelihood of the word “fantastic” given class positive, but suppose there are no training documents that both contain the word “fantastic” and are classified as positive. Perhaps the word “fantastic” happens to occur (sarcasti- cally?) in the class negative. In such a case the probability for this feature will be zero:
ˆ count (“fantastic”, positive) P(“fantastic”|positive) = 􏰄w∈V count (w, positive) = 0 (4.13)
But since naive Bayes naively multiplies all the feature likelihoods together, zero probabilities in the likelihood term for any class will cause the probability of the class to be zero, no matter the other evidence!
The simplest solution is the add-one (Laplace) smoothing introduced in Chap- ter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing

6 CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION
algorithms in language modeling, it is commonly used in naive Bayes text catego-
rization:
ˆ count(wi,c)+1 count(wi,c)+1
P(wi |c) = 􏰄w∈V (count (w, c) + 1) = 􏰀􏰄w∈V count (w, c)􏰁 + |V | (4.14)
  unknown word
stop words
Note once again that it is crucial that the vocabulary V consists of the union of all the word types in all classes, not just the words in one class c (try to convince yourself why this must be true; see the exercise at the end of the chapter).
What do we do about words that occur in our test data but are not in our vocab- ulary at all because they did not occur in any training document in any class? The solution for such unknown words is to ignore them—remove them from the test document and not include any probability for them at all.
Finally, some systems choose to completely ignore another class of words: stop words, very frequent words like the and a. This can be done by sorting the vocabu- lary by frequency in the training set, and defining the top 10–100 vocabulary entries as stop words, or alternatively by using one of the many pre-defined stop word list available online. Then every instance of these stop words are simply removed from both training and test documents as if they had never occurred. In most text classi- fication applications, however, using a stop word list doesn’t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list.
Fig. 4.2 shows the final algorithm.
   function TRAIN NAIVE BAYES(D, C) returns log P(c) and log P(w|c)
for each class c ∈ C # Calculate P(c) terms Ndoc = number of documents in D
Nc = number of documents from D in class c
logprior[c]← log Nc Ndoc
V←vocabulary of D
bigdoc[c]←append(d) for d ∈ D with class c
for each word w in V # Calculate P(w|c) terms
count(w,c)←# of occurrences of w in bigdoc[c] count(w,c) + 1
loglikelihood[w,c]← log 􏰄w′ in V (count (w′,c) + 1) return logprior, loglikelihood, V
 function TEST NAIVE BAYES(testdoc, logprior, loglikelihood, C, V) returns best c
for each class c ∈ C
sum[c]← logprior[c]
for each position i in testdoc
word ← testdoc[i] if word ∈ V
sum[c]←sum[c]+ loglikelihood[word,c] return argmaxc sum[c]
  Figure 4.2
The naive Bayes algorithm, using add-1 smoothing. To use add-α smoothing instead, change the +1 to +α for loglikelihood counts in training.

4.3 Worked example
Let’s walk through an example of training and testing naive Bayes with add-one smoothing. We’ll use a sentiment analysis domain with the two classes positive (+) and negative (-), and take the following miniature training and test documents simplified from actual movie reviews.
  Cat
?
The prior P(c) for the two classes is computed via Eq. 4.11 as
P(−)= 3 P(+)= 2 55
Nc : Ndoc
The word with doesn’t occur in the training set, so we drop it completely (as mentioned above, we don’t use unknown word models for naive Bayes). The like- lihoods from the training set for the remaining three words “predictable”, “no”, and “fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder of the words in the training set is left as an exercise for the reader):
P(“predictable”|−) = P(“no”|−) = P(“fun”|−) =
1 + 1 14+20 1+1 14+20 0 + 1 14+20
P(“predictable”|+) =
0 + 1 9+20
P(“fun”|+) =
For the test sentence S = “predictable with no fun”, after removing the word ‘with’,
9+20 the chosen class, via Eq. 4.9, is therefore computed as follows:
P(−)P(S|−) = 3 × 2×2×1 = 6.1×10−5 5 343
P(+)P(S|+) = 2 × 1×1×2 = 3.2×10−5 5 293
The model thus predicts the class negative for the test sentence.
4.4 Optimizing for Sentiment Analysis
While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance.
First, for sentiment classification and a number of other text classification tasks, whether a word occurs or not seems to matter more than its frequency. Thus it often improves performance to clip the word counts in each document at 1 (see the end of the chapter for pointers to these results). This variant is called binary
4.3 • WORKED EXAMPLE 7
 Documents
   Training
-
-
-
+ +
just plain boring
entirely predictable and lacks energy no surprises and very few laughs very powerful
the most fun film of the summer
   Test
predictable with no fun
P(“no”|+) =
0+1 9+20 1 + 1

8 CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION
binary NB
multinomial naive Bayes or binary NB. The variant uses the same Eq. 4.10 except that for each document we remove all duplicate words before concatenating them into the single big document. Fig. 4.3 shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary, with the modified counts shown in the table on the right. The example is worked without add-1 smoothing to make the differences clearer. Note that the results counts need not be 1; the word great has a count of 2 even for Binary NB, because it appears in multiple documents.
   Four original documents:
− it was pathetic the worst part was the boxing scenes
− no plot twists or great scenes
+ and satire and great plot twists
+ great scenes great film
After per-document binarization:
− it was pathetic the worst part boxing scenes
− no plot twists or great scenes
+ and satire great plot twists
+ great scenes film
NB Binary Counts Counts +−+−
and 2010 boxing 0 1 0 1 film 1010 great 3121 it 0101 no 0101 or 0101 part 0101 pathetic 0 1 0 1 plot 1111 satire 1010
   scenes 1
the 0
twists 1
was 0201 worst 0 1 0 1
2 1 2 2 0 1 1 1 1
    Figure 4.3
An example of binarization for the binary naive Bayes algorithm.
A second important addition commonly made when doing text classification for sentiment is to deal with negation. Consider the difference between I really like this movie (positive) and I didn’t like this movie (negative). The negation expressed by didn’t completely alters the inferences we draw from the predicate like. Similarly, negation can modify a negative word to produce a positive review (don’t dismiss this film, doesn’t let us get bored).
A very simple baseline that is commonly used in sentiment analysis to deal with negation is the following: during text normalization, prepend the prefix NOT to every word after a token of logical negation (n’t, not, no, never) until the next punc- tuation mark. Thus the phrase
      didnt like this movie , but I
becomes
didnt NOT_like NOT_this NOT_movie , but I
Newly formed ‘words’ like NOT like, NOT recommend will thus occur more of- ten in negative document and act as cues for negative sentiment, while words like NOT bored, NOT dismiss will acquire positive associations. We will return in Chap- ter 17 to the use of parsing to deal more accurately with the scope relationship be- tween these negation words and the predicates they modify, but this simple baseline works quite well in practice.
Finally, in some situations we might have insufficient labeled training data to train accurate naive Bayes classifiers using all words in the training set to estimate positive and negative sentiment. In such cases we can instead derive the positive

4.5
LIWC of Hu and Liu (2004) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). For example the MPQA subjectivity lexicon has 6885 words, 2718 positive and 4912 negative, each marked for whether it is strongly or weakly biased. (Chapter 21 will discuss how these lexicons can be learned automatically.) Some samples of
positive and negative words from the MPQA lexicon include:
+ : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great
− : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate
A common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs. Thus we might add a feature called ‘this word occurs in the positive lexicon’, and treat all instances of words in the lexicon as counts for that one feature, instead of counting each word separately. Similarly, we might add as a second feature ‘this word occurs in the negative lexicon’ of words in the negative lexicon. If we have lots of training data, and if the test data matches the training data, using just two features won’t work as well as using all the words. But when training data is sparse or not representative of the test set, using dense lexicon features instead of sparse individual-word features may generalize better.
Naive Bayes for other text classification tasks
In the previous section we pointed out that naive Bayes doesn’t require that our classifier use all the words in the training data as features. In fact features in naive Bayes can express any property of the input text we want.
Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email) — and one of the first applications of naive Bayes to text classification (Sahami et al., 1998).
A common solution here, rather than using all the words as individual features, is to predefine likely sets of words or phrases as features, combined these with features that are not purely linguistic. For example the open-source SpamAssassin tool1 predefines features like the phrase “one hundred percent guaranteed”, or the feature mentions millions of dollars, which is a regular expression that matches suspiciously large sums of money. But it also includes features like HTML has a low ratio of text to image area, that isn’t purely linguistic and might require some sophisticated computation, or totally non-linguistic features about, say, the path that the email took to arrive. More sample SpamAssassin features:
• Email subject line is all capital letters
• Contains phrases of urgency like “urgent reply”
• Email subject line contains “online pharmaceutical” • HTML has unbalanced ”head” tags
• Claims you can be removed from the list
For other tasks, like language ID—determining what language a given piece of text is written in—the most effective naive Bayes features are not words at all, but byte n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie z’, ‘thei’).
1 https://spamassassin.apache.org
sentiment lexicons
General Inquirer
4.5 • NAIVE BAYES FOR OTHER TEXT CLASSIFICATION TASKS 9
and negative word features from sentiment lexicons, lists of words that are pre- annotated with positive or negative sentiment. Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon
spam detection
language ID

10
CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION
Because spaces count as a byte, byte n-grams can model statistics about the begin-
2
Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia text in 68 different languages were used in (Lui and Baldwin, 2011)), or newswire. To make sure that this multilingual text correctly reflects different regions, dialects, and socioeconomic classes, systems also add Twitter text in many languages geo- tagged to many regions (important for getting world English dialects from countries with large Anglophone populations like Nigeria or India), Bible and Quran transla- tions, slang websites like Urban Dictionary, corpora of African American Vernacular English (Blodgett et al., 2016), and so on (Jurgens et al., 2017).
Naive Bayes as a Language Model
As we saw in the previous section, naive Bayes classifiers can use any sort of fea- ture: dictionaries, URLs, email addresses, network features, phrases, and so on. But if, as in the previous section, we use only individual word features, and we use all of the words in the text (not a subset), then naive Bayes has an important similar- ity to language modeling. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model.
Since the likelihood features from the naive Bayes model assign a probability to each word P(word|c), the model also assigns a probability to each sentence:
P(s|c) = 􏰆 P(wi|c) (4.15) i∈ posit ions
Thus consider a naive Bayes model with the classes positive (+) and negative (-) and the following model parameters:
w
I love this fun film ...
Each of the two columns above instantiates a language model that can assign a probability to the sentence “I love this fun film”:
P(”Ilovethisfunfilm”|+) = 0.1×0.1×0.01×0.05×0.1=0.0000005 P(”Ilovethisfunfilm”|−) = 0.2×0.001×0.01×0.005×0.1=.0000000010
As it happens, the positive model assigns a higher probability to the sentence: P(s|pos) > P(s|neg). Note that this is just the likelihood part of the naive Bayes
2 It’s also possible to use codepoints, which are multi-byte Unicode representations of characters in character sets, but simply using bytes seems to work better.
4.6
ning or ending of words.
and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using feature selection to winnow down to the most informative 7000 final features.
A widely used naive Bayes system, langid.py (Lui
 P(w|+) P(w|-)
 0.1 0.2
 0.1 0.001
 0.01 0.01
 0.05 0.005
 0.1 0.1
 ... ...

4.7 • EVALUATION: PRECISION, RECALL, F-MEASURE 11 model; once we multiply in the prior a full naive Bayes model might well make a
different classification decision.
4.7 Evaluation: Precision, Recall, F-measure
gold labels
contingency table
To introduce the methods for evaluating text classification, let’s first consider some simple binary detection tasks. For example, in spam detection, our goal is to label every text as being in the spam category (“positive”) or not in the spam category (“negative”). For each item (email document) we therefore need to know whether our system called it spam or not. We also need to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to match. We will refer to these human labels as the gold labels.
Or imagine you’re the CEO of the Delicious Pie Company and you need to know what people are saying about your pies on social media, so you build a system that detects tweets concerning Delicious Pie. Here the positive class is tweets about Delicious Pie and the negative class is all other tweets.
In both cases, we need a metric for knowing how well our spam detector (or pie-tweet-detector) is doing. To evaluate any system for detecting things, we start by building a contingency table like the one shown in Fig. 4.4. Each cell labels a set of possible outcomes. In the spam detection case, for example, true positives are documents that are indeed spam (indicated by human-created gold labels) and our system said they were spam. False negatives are documents that are indeed spam but our system labeled as non-spam.
To the bottom right of the table is the equation for accuracy, which asks what percentage of all the observations (for the spam or pie examples that means all emails or tweets) our system labeled correctly. Although accuracy might seem a natural metric, we generally don’t use it. That’s because accuracy doesn’t work well when the classes are unbalanced (as indeed they are with spam, which is a large majority of email, or with tweets, which are mainly not about pie).
     system output labels
system positive
system negative
recall =
tp precision = tp+fp
tp+tn tp+fp+tn+fn
gold standard labels
gold positive
gold negative
 true positive
 false positive
 false negative
 true negative
  tp tp+fn
accuracy =
   Figure 4.4
Contingency table
To make this more explicit, imagine that we looked at a million tweets, and let’s say that only 100 of them are discussing their love (or hatred) for our pie, while the other 999,900 are tweets about something completely unrelated. Imagine a simple classifier that stupidly classified every tweet as “not about pie”. This classifier would have 999,900 true negatives and only 100 false negatives for an accuracy of 999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should be happy with this classifier? But of course this fabulous ‘no pie’ classifier would

12
CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION
precision
recall
be completely useless, since it wouldn’t find a single one of the customer comments we are looking for. In other words, accuracy is not a good metric when the goal is to discover something that is rare, or at least not completely balanced in frequency, which is a very common situation in the world.
That’s why instead of accuracy we generally turn to two other metrics: precision and recall. Precision measures the percentage of the items that the system detected (i.e., the system labeled as positive) that are in fact positive (i.e., are positive accord- ing to the human gold labels). Precision is defined as
Precision = true positives
true positives + false positives
Recall measures the percentage of items actually present in the input that were correctly identified by the system. Recall is defined as
Recall = true positives
true positives + false negatives
Precision and recall will help solve the problem with the useless “nothing is pie” classifier. This classifier, despite having a fabulous accuracy of 99.99%, has a terrible recall of 0 (since there are no true positives, and 100 false negatives, the recall is 0/100). You should convince yourself that the precision at finding relevant tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.
There are many ways to define a single metric that incorporates aspects of both precision and recall. The simplest of these combinations is the F-measure (van Rijsbergen, 1975) , defined as:
Fβ =(β2+1)PR β2P+R
The β parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of β > 1 favor recall, while values of β < 1 favor precision. When β = 1, precision and recall are equally bal- anced; this is the most frequently used metric, and is called Fβ=1 or just F1:
F1 = 2PR (4.16) P+R
F-measure comes from a weighted harmonic mean of precision and recall. The harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip- rocals:
  F-measure
F1
 HarmonicMean(a1,a2,a3,a4,...,an) = n
1 + 1 + 1 +...+ 1
(4.17)
(β 2 + 1)PR
F=2 (4.18)
Harmonic mean is used because it is a conservative metric; the harmonic mean of two values is closer to the minimum of the two values than the arithmetic mean is. Thus it weighs the lower of the two numbers more heavily.
 and hence F-measure is
F=1
β P+R
1 αP+(1−α)R
􏰂 2
1 orwithβ=
a1 a2
1 − α 􏰃 α
a3 an

any-of
one-of
multinomial classification
4.7 • EVALUATION: PRECISION, RECALL, F-MEASURE 13 4.7.1 More than two classes
Up to now we have been assuming text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on.
There are two kinds of multi-class classification tasks. In any-of or multi-label classification, each document or item can be assigned more than one label. We can solve any-of classification by building separate binary classifiers for each class c, trained on positive examples labeled c and negative examples not labeled c. Given a test document or item d, then each classifier makes their decision independently, and we may assign multiple labels to d.
More common in language processing is one-of or multinomial classification, in which the classes are mutually exclusive and each document or item appears in exactly one class. Here we again build a separate binary classifier trained on positive examples from c and negative examples from all other classes. Now given a test document or item d, we run all the classifiers and choose the label from the classifier with the highest score. Consider the sample confusion matrix for a hypothetical 3- way one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.5.
    urgent urgent
system
output normal
spam
recallu =
spam
gold labels
normal
8 precisionu= 8+10+1
60 precisionn= 5+60+50
200 precisions= 3+30+200
 8
 10
 1
 5
 60
 50
 3
 30
 200
      recalln = recalls =
8 60 200 8+5+3 10+60+30 1+50+200
  Figure 4.5
macroaveraging microaveraging
Confusion matrix for a three-class categorization task, showing for each pair of classes (c1,c2), how many documents from c1 were (in)correctly assigned to c2
The matrix shows, for example, that the system mistakenly labeled 1 spam doc- ument as urgent, and we have shown how to compute a distinct precision and recall value for each class. In order to derive a single metric that tells us how well the system is doing, we can combine these values in two ways. In macroaveraging, we compute the performance for each class, and then average over classes. In microav- eraging, we collect the decisions for all classes into a single contingency table, and then compute precision and recall from that table. Fig. 4.6 shows the contingency table for each class separately, and shows the computation of microaveraged and macroaveraged precision.
As the figure shows, a microaverage is dominated by the more frequent class (in this case spam), since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important.

14 CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION
    Class 1: Urgent
true true urgent not
system
urgent 8 11
system 8 340 not
Class 2: Normal
true true normal not
system
normal 60 55
system 40 212 not
Class 3: Spam Pooled
true true true true spam not yes no
system system
spam 200 33 yes 268 99
system 51 83 system 99 635
precision =
= .42
precision =
= .52
microaverage 268
8
8+11
60 60+55
not
precision = = .60
200 200+33
no
268+99
macroaverage precision
=
.42+.52+.86 3
= .86 = precision
= .73
   Figure 4.6
Separate contingency tables for the 3 classes from the previous figure, showing the pooled contin- gency table and the microaveraged and macroaveraged precision.
4.8 Test sets and Cross-validation
development test set
devset
cross-validation
10-fold cross-validation
The training and testing procedure for text classification follows what we saw with language modeling (Section ??): we use the training set to train the model, then use the development test set (also called a devset) to perhaps tune some parameters, and in general decide what the best model is. Once we come up with what we think is the best model, we run it on the (hitherto unseen) test set to report its performance.
While the use of a devset avoids overfitting the test set, having a fixed training set, devset, and test set creates another problem: in order to save lots of data for training, the test set (or devset) might not be large enough to be representative. It would be better if we could somehow use all our data both for training and test. We do this by cross-validation: we randomly choose a training and test set division of our data, train our classifier, and then compute the error rate on the test set. Then we repeat with a different randomly selected training set and test set. We do this sampling process 10 times and average these 10 runs to get an average error rate. This is called 10-fold cross-validation.
The only problem with cross-validation is that because all the data is used for testing, we need the whole corpus to be blind; we can’t examine any of the data to suggest possible features and in general see what’s going on. But looking at the corpus is often important for designing the system. For this reason, it is common to create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set, as shown in Fig. 4.7.
4.9 Statistical Significance Testing
In building systems we are constantly comparing the performance of systems. Often we have added some new bells and whistles to our algorithm and want to compare the new version of the system to the unaugmented version. Or we want to compare our algorithm to a previously published one to know which is better.
We might imagine that to compare the performance of two classifiers A and B all we have to do is look at A and B’s score on the same test set—for example we might choose to compare macro-averaged F1— and see whether it’s A or B that has

null hypothesis
10-fold cross-validation
the higher score. But just looking at this one difference isn’t good enough, because A might have a better performance than B on a particular test set just by chance.
Let’s say we have a test set x of n observations x = x1,x2,..,xn on which A’s performance is better than B by δ (x). How can we know if A is really better than B? To do so we’d need to reject the null hypothesis that A isn’t really better than B and this difference δ(x) occurred purely by chance. If the null hypothesis was correct, we would expect that if we had many test sets of size n and we measured A and B’s performance on all of them, that on average A might accidentally still be better than B by this amount δ (x) just by chance.
More formally, if we had a random variable X ranging over test sets, the null hypothesis H0 expects P(δ(X) > δ(x)|H0), the probability that we’ll see similarly big differences just by chance, to be high.
′′
In language processing we don’t generally use traditional statistical approaches like paired t-tests to compare system outputs because most metrics are not normally distributed, violating the assumptions of the tests. The standard approach to comput- ing p-value(x) in natural language processing is to use non-parametric tests like the bootstrap test (Efron and Tibshirani, 1993)— which we will describe below—or a similar test, approximate randomization (Noreen, 1989). The advantage of these tests is that they can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation.
The word bootstrapping refers to repeatedly drawing large numbers of smaller samples with replacement (called bootstrap samples) from an original larger sam- ple. The intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is representative of the population.
Consider a tiny text classification example with a test set x of 10 documents. The first row of Fig. 4.8 shows the results of two classifiers (A and B) on this test set, with each document labeled by one of the four possibilities: (A and B both right, both wrong, A right and B wrong, A wrong and B right); a slash through a letter
bootstrap test
approximate randomization
bootstrapping
4.9 • STATISTICAL SIGNIFICANCE TESTING 15
    Training Iterations 1
2
3 4 5 6 7 8 9 10
Testing
 Dev Training
 Dev Training
 Dev Training
 Dev Training
 Training Dev Training
 Training Dev
 Training Dev
 Training Dev
 Training Dev
 Training Dev
Test Set
  Figure 4.7
If we had all these test sets we could just measure all the δ(x ) for all the x . If we found that those deltas didn’t seem to be bigger than δ (x), that is, that p-value(x) was sufficiently small, less than the standard thresholds of 0.05 or 0.01, then we might reject the null hypothesis and agree that δ (x) was a sufficiently surprising difference and A is really a better algorithm than B. Following Berg-Kirkpatrick et al. (2012) we’ll refer to P(δ (X ) > δ (x)|H0 ) as p-value(x).

16 CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION
(􏰌B) means that that classifier got the answer wrong. On the first document both A and B get the correct class (AB), while on the second document A got it right but B got it wrong (A􏰌B). If we assume for simplicity that our metric is accuracy, A has an accuracy of .70 and B of .50, so δ(x) is .20. To create each virtual test set of size N = 10, we repeatedly (10 times) select a cell from row x with replacement. Fig. 4.8 shows a few examples.
1 2 3 4 5 6 7 8 9 10A%B%δ()
x AB AB􏰌􏰌 AB 􏰋􏰋AB AB􏰌􏰌 􏰋􏰋AB AB􏰌􏰌 AB 􏰋􏰋AB􏰌􏰌 AB􏰌􏰌 .70 .50 .20
x∗(1) AB􏰌􏰌 AB AB􏰌􏰌 􏰋􏰋AB 􏰋􏰋AB AB􏰌􏰌 􏰋􏰋AB AB 􏰋􏰋AB􏰌􏰌 AB .60 .60 .00
x∗(2) AB􏰌􏰌 AB 􏰋􏰋AB􏰌􏰌 􏰋􏰋AB 􏰋􏰋AB AB 􏰋􏰋AB AB􏰌􏰌 AB AB .60 .70 -.10
...
x∗(b)
Figure 4.8 The bootstrap: Examples of b pseudo test sets being created from an initial true test set x. Each pseudo test set is created by sampling n = 10 times with replacement; thus an individual sample is a single cell, a document with its gold label and the correct or incorrect performance of classifiers A and B.
Now that we have a sampling distribution, we can do statistics on how often A has an accidental advantage. There are various ways to compute this advantage; here we follow the version laid out in Berg-Kirkpatrick et al. (2012). We might think that we should just ask, for each bootstrap sample x∗(i), whether A beats B by more than δ (x). But there’s a problem: we didn’t draw these samples from a distribution with 0 mean. The x∗(i) were sampled from x, and so the expected value of δ(x∗(i)) lies very close to δ (x). That is, about half the time A will be better than B, so we expect A to beat B by δ (x). Instead, we want to know how often A beats these expectations by more than δ(x). To correct for the expected success, we need to zero-center, subtracting δ (x) from each pseudo test set. Thus we’ll be comparing for each x∗(i) whether δ(x∗(i)) > 2δ(x). The full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set x, a number of samples b, and counts the percentage of the b bootstrap test sets in which δ(x∗(i)) > 2δ(x). This percentage then acts as a one- sided empirical p-value (more sophisticated ways to get p-values from confidence intervals also exist).
     function BOOTSTRAP(test set x, num of samples b) returns p-value(x)
Calculate δ (x) # how much better does algorithm A do than B on x fori= 1tobdo
forj=1tondo #Drawabootstrapsamplex∗(i) ofsizen Select a member of x at random and add it to x∗(i)
Calculate δ(x∗(i)) # how much better does algorithm A do than B on x∗(i) for each x∗(i)
s←s + 1 if δ(x∗(i)) > 2δ(x)
p-value(x) ≈ bs # on what % of the b samples did algorithm A beat expectations? return p-value(x)
  Figure 4.9
A version of the bootstrap algorithm after Berg-Kirkpatrick et al. (2012).

4.10 Summary
This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.
• Many language processing tasks can be viewed as tasks of classification.
• Textcategorization,inwhichanentiretextisassignedaclassfromafiniteset, includes such tasks as sentiment analysis, spam detection, language identi- fication, and authorship attribution.
• Sentiment analysis classifies a text as reflecting the positive or negative orien- tation (sentiment) that a writer expresses toward some object.
• Naive Bayes is a generative model that makes the bag of words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class)
• Naive Bayes with binarized features seems to work better for many text clas- sification tasks.
• Classifiers are evaluated based on precision and recall.
• Classifiers are trained using distinct training, dev, and test sets, including the
use of cross-validation in the training set.
Bibliographical and Historical Notes
Multinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, ap- proximating the classification task with one-of categorization, and implementing add-δ smoothing and information-based feature selection.
The conditional independence assumptions of naive Bayes and the idea of Bayes- ian analysis of text seems to have arisen multiple times. The same year as Maron’s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other arti- ficial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 1787–1788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998).
Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also use a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates P(w|c) as the fraction of documents that contain a term, and includes a probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive
4.10 • SUMMARY 17

18
CHAPTER 4 • NAIVE BAYES AND SENTIMENT CLASSIFICATION
information gain
Bayes doesn’t work as well as the multinomial algorithm for sentiment or other text tasks.
There are a variety of sources covering the many kinds of text classification tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012). Stamatatos (2009) surveys authorship attribute algorithms. On language identifica- tion see Jauhiainen et al. (2018); Jaech et al. (2016) is an important early neural system. The task of newswire indexing was often used as a test case for text classi- fication algorithms, based on the Reuters-21578 collection of newswire articles.
See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classification; classification in general is covered in machine learning textbooks (Hastie et al. 2001, Witten and Frank 2005, Bishop 2006, Murphy 2012).
Non-parametric methods for computing statistical significance were used first in NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work has focused on issues including multiple test sets and multiple metrics (Søgaard et al. 2014, Dror et al. 2017).
Feature selection is a method of removing features that are unlikely to generalize well. Features are generally ranked by how informative they are about the classifica- tion decision. A very common metric, information gain, tells us how many bits of information the presence of the word gives us for guessing the class. Other feature selection metrics include χ2, pointwise mutual information, and GINI index; see Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an introduction to feature selection.
Exercises
4.1 Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.
pos neg I 0.09 0.16 always 0.07 0.06 like 0.29 0.06 foreign 0.04 0.15 films 0.08 0.11
What class will Naive bayes assign to the sentence “I always like foreign films.”?
4.2 Given the following short movie reviews, each labeled with a genre, either comedy or action:
1. fun, couple, love, love comedy
2. fast, furious, shoot action
3. couple, fly, fast, fun, fun comedy
4. furious, shoot, shoot, fun action
5. fly, fast, shoot, love action
and a new document D: fast, couple, shoot, fly
compute the most likely class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods.

EXERCISES 19
4.3 Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.
doc “good” “poor” “great” (class) d1.3 0 3 pos d2.0 1 2 pos d3.1 3 0 neg d4.1 5 2 neg d5.0 2 0 neg
Use both naive Bayes models to assign a class (pos or neg) to this sentence: A good, good plot and great characters, but poor acting.
Do the two models agree or disagree?

20 Chapter 4 • Naive Bayes and Sentiment Classification
 Aggarwal, C. C. and Zhai, C. (2012). A survey of text classi- fication algorithms. In Aggarwal, C. C. and Zhai, C. (Eds.), Mining text data, 163–222. Springer.
Bayes, T. (1763). An Essay Toward Solving a Problem in the Doctrine of Chances, Vol. 53. Reprinted in Facsimiles of Two Papers by Bayes, Hafner Publishing, 1963.
Berg-Kirkpatrick, T., Burkett, D., and Klein, D. (2012). An empirical investigation of statistical significance in NLP. In EMNLP 2012, 995–1005.
Bisani, M. and Ney, H. (2004). Bootstrap estimates for confidence intervals in ASR performance evaluation. In ICASSP-04, Vol. I, 409–412.
Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.
Blodgett, S. L., Green, L., and O’Connor, B. (2016). Demo- graphic dialectal variation in social media: A case study of African-American English. In EMNLP 2016.
Borges, J. L. (1964). The analytical language of John Wilkins. University of Texas Press. Trans. Ruth L. C. Simms.
Chinchor, N., Hirschman, L., and Lewis, D. L. (1993). Eval- uating Message Understanding systems: An analysis of the third Message Understanding Conference. Computational Linguistics, 19(3), 409–449.
Dror, R., Baumer, G., Bogomolov, M., and Reichart, R. (2017). Replicability analysis for natural language process- ing: Testing significance with multiple datasets. TACL, 5, 471–486.
Efron, B. and Tibshirani, R. J. (1993). An introduction to the bootstrap. CRC press.
Gillick, L. and Cox, S. J. (1989). Some statistical issues in the comparison of speech recognition algorithms. In ICASSP-89, 532–535.
Guyon, I. and Elisseeff, A. (2003). An introduction to vari- able and feature selection. JMLR, 3, 1157–1182.
Hastie, T., Tibshirani, R. J., and Friedman, J. H. (2001). The Elements of Statistical Learning. Springer.
Heckerman, D., Horvitz, E., Sahami, M., and Dumais, S. T. (1998). A bayesian approach to filtering junk e-mail. In Proceeding of AAAI-98 Workshop on Learning for Text Categorization, 55–62.
Hu, M. and Liu, B. (2004). Mining and summarizing cus- tomer reviews. In KDD, 168–177.
Jaech, A., Mulcaire, G., Hathi, S., Ostendorf, M., and Smith, N. A. (2016). Hierarchical character-word models for lan- guage identification. In ACL Workshop on NLP for Social Media, 84–93.
Jauhiainen, T., Lui, M., Zampieri, M., Baldwin, T., and Linde ́n, K. (2018). Automatic language identification in texts: A survey. arXiv preprint arXiv:1804.08186.
Jurgens, D., Tsvetkov, Y., and Jurafsky, D. (2017). Incorpo- rating dialectal variability for socially equitable language identification. In ACL 2017, 51–57.
Liu, B. and Zhang, L. (2012). A survey of opinion mining and sentiment analysis. In Aggarwal, C. C. and Zhai, C. (Eds.), Mining text data, 415–464. Springer.
Lui, M. and Baldwin, T. (2011). Cross-domain feature selec- tion for language identification. In IJCNLP-11, 553–561.
Lui, M. and Baldwin, T. (2012). langid.py: An off-the- shelf language identification tool. In ACL 2012, 25–30.
Manning, C. D., Raghavan, P., and Schu ̈tze, H. (2008). In- troduction to Information Retrieval. Cambridge.
Maron, M. E. (1961). Automatic indexing: an experimental inquiry. Journal of the ACM (JACM), 8(3), 404–417.
McCallum, A. and Nigam, K. (1998). A comparison of event models for naive bayes text classification. In AAAI/ICML- 98 Workshop on Learning for Text Categorization, 41–48.
Metsis, V., Androutsopoulos, I., and Paliouras, G. (2006). Spam filtering with naive bayes-which naive bayes?. In CEAS, 27–28.
Minsky, M. (1961). Steps toward artificial intelligence. Pro- ceedings of the IRE, 49(1), 8–30.
Mosteller, F. and Wallace, D. L. (1963). Inference in an au- thorship problem: A comparative study of discrimination methods applied to the authorship of the disputed federal- ist papers. Journal of the American Statistical Association, 58(302), 275–309.
Mosteller, F. and Wallace, D. L. (1964). Inference and Dis- puted Authorship: The Federalist. Springer-Verlag. A second edition appeared in 1984 as Applied Bayesian and Classical Inference.
Murphy, K. P. (2012). Machine learning: A probabilistic perspective. MIT press.
Noreen, E. W. (1989). Computer Intensive Methods for Test- ing Hypothesis. Wiley.
Pang, B. and Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2), 1–135.
Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using machine learning tech- niques. In EMNLP 2002, 79–86.
Pennebaker, J. W., Booth, R. J., and Francis, M. E. (2007). Linguistic Inquiry and Word Count: LIWC 2007. Austin, TX.
Sahami, M., Dumais, S. T., Heckerman, D., and Horvitz, E. (1998). A Bayesian approach to filtering junk e-mail. In AAAI Workshop on Learning for Text Categorization, 98– 105.
Søgaard, A., Johannsen, A., Plank, B., Hovy, D., and Alonso, H. M. (2014). What’s in a p-value in NLP?. In CoNLL-14.
Stamatatos, E. (2009). A survey of modern authorship attri- bution methods. JASIST, 60(3), 538–556.
Stone, P., Dunphry, D., Smith, M., and Ogilvie, D. (1966).
The General Inquirer: A Computer Approach to Content Analysis. Cambridge, MA: MIT Press.
van Rijsbergen, C. J. (1975). Information Retrieval. Butter- worths.
Wang, S. and Manning, C. D. (2012). Baselines and bigrams: Simple, good sentiment and topic classification. In ACL 2012, 90–94.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recogniz- ing contextual polarity in phrase-level sentiment analysis. In HLT-EMNLP-05, 347–354.
Witten, I. H. and Frank, E. (2005). Data Mining: Practical Machine Learning Tools and Techniques (2nd Ed.). Mor- gan Kaufmann.

Exercises 21
 Yang, Y. and Pedersen, J. (1997). A comparative study on feature selection in text categorization. In ICML, 412–420.

 Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ⃝c 2019. All rights reserved. Draft of October 2, 2019.
 CHAPTER
8
parts of speech
Part-of-Speech Tagging
Dionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a “techne ̄”) that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including words like syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax’s set of eight that became the basis for practically all subsequent part-of-speech descriptions of most European languages for the next 2000 years.
Schoolhouse Rock was a series of popular animated educational television clips from the 1970s. Its Grammar Rock sequence included songs about exactly 8 parts of speech, including the late great Bob Dorough’s Conjunction Junction:
Conjunction Junction, what’s your function? Hooking up words and phrases and clauses...
Although the list of 8 was slightly modified from Thrax’s original, the astonishing durability of the parts of speech through two millennia is an indicator of both the
1
noun and conTENT when it is an adjective.
This chapter introduces parts of speech, and then introduces two algorithms for
part-of-speech tagging, the task of assigning parts of speech to words. One is generative— Hidden Markov Model (HMM)—and one is discriminative—the Max- imum Entropy Markov Model (MEMM). Chapter 9 then introduces a third algorithm based on the recurrent neural network (RNN). All three have roughly equal perfor- mance but, as we’ll see, have different tradeoffs.
8.1
(Mostly) English Word Classes
Until now we have been using part-of-speech terms like noun and verb rather freely. In this section we give a more complete definition of these and other classes. While word classes do have semantic tendencies—adjectives, for example, often describe
1 Nonetheless, eight isn’t very many and, as we’ll see, recent tagsets have more.
POS
importance and the transparency of their role in human language.
Parts of speech (also known as POS, word classes, or syntactic categories) are useful because they reveal a lot about a word and its neighbors. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns are pre- ceded by determiners and adjectives, verbs by nouns) and syntactic structure (nouns are generally part of noun phrases), making part-of-speech tagging a key aspect of parsing (Chapter 13). Parts of speech are useful features for labeling named entities like people or organizations in information extraction (Chapter 18), or for corefer- ence resolution (Chapter 22). A word’s part of speech can even play a role in speech recognition or synthesis, e.g., the word content is pronounced CONtent when it is a

2
CHAPTER 8 • PART-OF-SPEECH TAGGING
closed class open class
function word
noun
proper noun
common noun count noun mass noun
verb
adjective
adverb
properties and nouns people— parts of speech are traditionally defined instead based on syntactic and morphological function, grouping words that have similar neighbor- ing words (their distributional properties) or take similar affixes (their morpholog- ical properties).
Parts of speech can be divided into two broad supercategories: closed class types and open class types. Closed classes are those with relatively fixed membership, such as prepositions—new prepositions are rarely coined. By contrast, nouns and verbs are open classes—new nouns and verbs like iPhone or to fax are continually being created or borrowed. Any given speaker or corpus may have different open class words, but all speakers of a language, and sufficiently large corpora, likely share the set of closed class words. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar.
Four major open classes occur in the languages of the world: nouns, verbs, adjectives, and adverbs. English has all four, although not every language does. The syntactic class noun includes the words for most people, places, or things, but others as well. Nouns include concrete terms like ship and chair, abstractions like bandwidth and relationship, and verb-like terms like pacing as in His pacing to and fro became quite annoying. What defines a noun in English, then, are things like its ability to occur with determiners (a goat, its bandwidth, Plato’s Republic), to take possessives (IBM’s annual revenue), and for most but not all nouns to occur in the plural form (goats, abaci).
Open class nouns fall into two classes. Proper nouns, like Regina, Colorado, and IBM, are names of specific persons or entities. In English, they generally aren’t preceded by articles (e.g., the book is upstairs, but Regina is upstairs). In written English, proper nouns are usually capitalized. The other class, common nouns, are divided in many languages, including English, into count nouns and mass nouns. Count nouns allow grammatical enumeration, occurring in both the singular and plu- ral (goat/goats, relationship/relationships) and they can be counted (one goat, two goats). Mass nouns are used when something is conceptualized as a homogeneous group. So words like snow, salt, and communism are not counted (i.e., *two snows or *two communisms). Mass nouns can also appear without articles where singular count nouns cannot (Snow is white but not *Goat is white).
Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-sg (eat), third-person-sg (eats), progressive (eating), past participle (eaten)). While many researchers believe that all human languages have the categories of noun and verb, others have argued that some languages, such as Riau Indonesian and Tongan, don’t even make this distinction (Broschart 1997; Evans 2000; Gil 2000) .
The third open class English form is adjectives, a class that includes many terms for properties or qualities. Most languages have adjectives for the concepts of color (white, black), age (old, young), and value (good, bad), but there are languages without adjectives. In Korean, for example, the words corresponding to English adjectives act as a subclass of verbs, so what is in English an adjective “beautiful” acts in Korean like a verb meaning “to be beautiful”.
The final open class form, adverbs, is rather a hodge-podge in both form and meaning. In the following all the italicized words are adverbs:
Actually, I ran home extremely quickly yesterday
What coherence the class has semantically may be solely that each of these words can be viewed as modifying something (often verbs, hence the name “ad-

locative degree manner temporal
8.1 • (MOSTLY) ENGLISH WORD CLASSES 3
verb”, but also other adverbs and entire verb phrases). Directional adverbs or loca- tive adverbs (home, here, downhill) specify the direction or location of some action; degree adverbs (extremely, very, somewhat) specify the extent of some action, pro- cess, or property; manner adverbs (slowly, slinkily, delicately) describe the manner of some action or process; and temporal adverbs describe the time that some ac- tion or event took place (yesterday, Monday). Because of the heterogeneous nature of this class, some adverbs (e.g., temporal adverbs like Monday) are tagged in some tagging schemes as nouns.
The closed classes differ more from language to language than do the open classes. Some of the important closed classes in English include:
prepositions: on, under, over, near, by, at, from, to, with particles: up, down, on, off, in, out, at, by determiners: a, an, the
conjunctions: and, but, or, as, if, when
pronouns: she, who, I, others
auxiliary verbs: can, may, should, are numerals: one, two, three, first, second, third
Prepositions occur before noun phrases. Semantically they often indicate spatial or temporal relations, whether literal (on it, before then, by the house) or metaphor- ical (on time, with gusto, beside herself), but often indicate other relations as well, like marking the agent in Hamlet was written by Shakespeare. A particle resembles a preposition or an adverb and is used in combination with a verb. Particles often have extended meanings that aren’t quite the same as the prepositions they resemble, as in the particle over in she turned the paper over.
A verb and a particle that act as a single syntactic and/or semantic unit are called a phrasal verb. The meaning of phrasal verbs is often problematically non- compositional—not predictable from the distinct meanings of the verb and the par- ticle. Thus, turn down means something like ‘reject’, rule out ‘eliminate’, find out ‘discover’, and go on ‘continue’.
A closed class that occurs with nouns, often marking the beginning of a noun phrase, is the determiner. One small subtype of determiners is the article: English has three articles: a, an, and the. Other determiners include this and that (this chap- ter, that page). A and an mark a noun phrase as indefinite, while the can mark it as definite; definiteness is a discourse property (Chapter 23). Articles are quite fre- quent in English; indeed, the is the most frequently occurring word in most corpora of written English, and a and an are generally right behind.
Conjunctions join two phrases, clauses, or sentences. Coordinating conjunc- tions like and, or, and but join two elements of equal status. Subordinating conjunc- tions are used when one of the elements has some embedded status. For example, that in “I thought that you might like some milk” is a subordinating conjunction that links the main clause I thought with the subordinate clause you might like some milk. This clause is called subordinate because this entire clause is the “content” of the main verb thought. Subordinating conjunctions like that which link a verb to its argument in this way are also called complementizers.
Pronouns are forms that often act as a kind of shorthand for referring to some noun phrase or entity or event. Personal pronouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are forms of personal pronouns that in- dicate either actual possession or more often just an abstract relation between the person and some object (my, your, his, her, its, one’s, our, their). Wh-pronouns (what, who, whom, whoever) are used in certain question forms, or may also act as
preposition
particle
phrasal verb
determiner article
conjunctions
complementizer pronoun
personal possessive
wh

4
CHAPTER 8 • PART-OF-SPEECH TAGGING
8.2
The Penn Treebank Part-of-Speech Tagset
An important tagset for English is the 45-tag Penn Treebank tagset (Marcus et al., 1993), shown in Fig. 8.1, which has been used to label many corpora. In such labelings, parts of speech are generally represented by placing the tag after each word, delimited by a slash:
auxiliary
copula modal
interjection negative
complementizers (Frida, who married Diego. . . ).
A closed class subtype of English verbs are the auxiliary verbs. Cross-linguist-
ically, auxiliaries mark semantic features of a main verb: whether an action takes place in the present, past, or future (tense), whether it is completed (aspect), whether it is negated (polarity), and whether an action is necessary, possible, suggested, or desired (mood). English auxiliaries include the copula verb be, the two verbs do and have, along with their inflected forms, as well as a class of modal verbs. Be is called a copula because it connects subjects with certain kinds of predicate nominals and adjectives (He is a duck). The verb have can mark the perfect tenses (I have gone, I had gone), and be is used as part of the passive (We were robbed) or progressive (We are leaving) constructions. Modals are used to mark the mood associated with the event depicted by the main verb: can indicates ability or possibility, may permission or possibility, must necessity. There is also a modal use of have (e.g., I have to go).
English also has many words of more or less unique function, including inter- jections (oh, hey, alas, uh, um), negatives (no, not), politeness markers (please, thank you), greetings (hello, goodbye), and the existential there (there are two on the table) among others. These classes may be distinguished or lumped together as interjections or adverbs depending on the purpose of the labeling.
 Tag Description Example Tag Description Example Tag Description Example
 CC coordinating conjunction
CD cardinal number DT determiner
EX existential ‘there’ FW foreign word
IN preposition/ subordin-conj
JJ adjective
JJR comparative adj
JJS superlative adj
LS list item marker MD modal
NN sing or mass noun NNS noun, plural NNP proper noun, sing. NNPS proper noun, plu.
and, but, or
one, two a, the there
mea culpa of, in, by
yellow bigger wildest
1, 2, One can, should llama llamas IBM Carolinas
PDT predeterminer
POS possessive ending PRP personal pronoun PRP$ possess. pronoun RB adverb
RBR comparative adverb
RBS superlatv. adverb RP particle
SYM symbol
TO “to”
UH interjection VB verb base form VBD verb past tense VBG verb gerund VBN verb past part.
all, both VBP ’s VBZ
I, you, he WDT your, one’s WP quickly WP$ faster WRB
fastest $ up, off # +,%, & “ to ” ah, oops ( eat ) ate , eating . eaten :
verb non-3sg present
verb 3sg pres wh-determ. wh-pronoun wh-possess. wh-adverb
dollar sign pound sign left quote right quote left paren right paren comma sent-end punc sent-mid punc
eat
eats which, that what, who whose how, where
$
#
‘or“ ’or” [,(,{,< ],),},> ,
.!?
: ;... –-
  Figure 8.1
 Penn Treebank part-of-speech tags (including punctuation).
(8.1) The/DTgrand/JJjury/NNcommented/VBDon/INa/DTnumber/NNof/IN other/JJ topics/NNS ./.
(8.2) There/EXare/VBP70/CDchildren/NNSthere/RB

Brown
WSJ Switchboard
8.2 • THE PENN TREEBANK PART-OF-SPEECH TAGSET 5 (8.3) Preliminary/JJfindings/NNSwere/VBDreported/VBNin/INtoday/NN
’s/POS New/NNP England/NNP Journal/NNP of/IN Medicine/NNP ./.
Example (8.1) shows the determiners the and a, the adjectives grand and other, the common nouns jury, number, and topics, and the past tense verb commented. Example (8.2) shows the use of the EX tag to mark the existential there construction in English, and, for comparison, another use of there which is tagged as an adverb (RB). Example (8.3) shows the segmentation of the possessive morpheme ’s, and a passive construction, ‘were reported’, in which reported is tagged as a past participle (VBN). Note that since New England Journal of Medicine is a proper noun, the Treebank tagging chooses to mark each noun in it separately as NNP, including journal and medicine, which might otherwise be labeled as common nouns (NN).
Corpora labeled with parts of speech are crucial training (and testing) sets for statistical tagging algorithms. Three main tagged corpora are consistently used for training and testing part-of-speech taggers for English. The Brown corpus is a mil- lion words of samples from 500 written texts from different genres published in the United States in 1961. The WSJ corpus contains a million words published in the Wall Street Journal in 1989. The Switchboard corpus consists of 2 million words of telephone conversations collected in 1990-1991. The corpora were created by running an automatic part-of-speech tagger on the texts and then human annotators hand-corrected each tag.
There are some minor differences in the tagsets used by the corpora. For example in the WSJ and Brown corpora, the single Penn tag TO is used for both the infinitive to (I like to race) and the preposition to (go to the store), while in Switchboard the tag TO is reserved for the infinitive use of to and the preposition is tagged IN:
Well/UH ,/, I/PRP ,/, I/PRP want/VBP to/TO go/VB to/IN a/DT restaurant/NN
Finally, there are some idiosyncracies inherent in any tagset. For example, be- cause the Penn 45 tags were collapsed from a larger 87-tag tagset, the original Brown tagset, some potentially useful distinctions were lost. The Penn tagset was designed for a treebank in which sentences were parsed, and so it leaves off syntactic information recoverable from the parse tree. Thus for example the Penn tag IN is used for both subordinating conjunctions like if, when, unless, after:
after/IN spending/VBG a/DT day/NN at/IN the/DT beach/NN and prepositions like in, on, after:
after/IN sunrise/NN
Words are generally tokenized before tagging. The Penn Treebank and the
British National Corpus split contractions and the ’s-genitive from their stems:2 would/MD n’t/RB
children/NNS ’s/POS
The Treebank tagset assumes that tokenization of multipart words like New York is done at whitespace, thus tagging. a New York City firm as a/DT New/NNP York/NNP City/NNP firm/NN.
Another commonly used tagset, the Universal POS tag set of the Universal De- pendencies project (Nivre et al., 2016), is used when building systems that can tag many languages. See Section 8.7.
2 Indeed, the Treebank tag POS is used only for ’s, which must be segmented in tokenization.

6 CHAPTER 8 • PART-OF-SPEECH TAGGING 8.3 Part-of-Speech Tagging
part-of-speech tagging
ambiguous
ambiguity resolution
Part-of-speech tagging is the process of assigning a part-of-speech marker to each
3
word tokens in running text are ambiguous.
The input to a tagging algorithm is a sequence of (tokenized)
word in an input text.
words and a tagset, and the output is a sequence of tags, one per token.
Tagging is a disambiguation task; words are ambiguous —have more than one
possible part-of-speech—and the goal is to find the correct tag for the situation.
For example, book can be a verb (book that flight) or a noun (hand me that book).
That can be a determiner (Does that flight serve dinner) or a complementizer (I
thought that your flight was earlier). The goal of POS-tagging is to resolve these
ambiguities, choosing the proper tag for the context. How common is tag ambiguity?
Fig. 8.2 shows that most word types (85-86%) are unambiguous (Janet is always
NNP, funniest JJS, and hesitantly RB). But the ambiguous words, though accounting
for only 14-15% of the vocabulary, are very common words, and hence 55-67% of
4
Tag ambiguity for word types in Brown and WSJ, using Treebank-3 (45-tag) tagging. Punctuation were treated as words, and words were kept in their original case.
Some of the most ambiguous frequent words are that, back, down, put and set; here are some examples of the 6 different parts of speech for the word back:
earnings growth took a back/JJ seat
a small building in the back/NN
a clear majority of senators back/VBP the bill Dave began to back/VB toward the door enable the country to buy back/RP about debt I was twenty-one back/RB then
Nonetheless, many words are easy to disambiguate, because their different tags aren’t equally likely. For example, a can be a determiner or the letter a, but the determiner sense is much more likely. This idea suggests a simplistic baseline algo- rithm for part-of-speech tagging: given an ambiguous word, choose the tag which is most frequent in the training corpus. This is a key concept:
How good is this baseline? A standard way to measure the performance of part- of-speech taggers is accuracy: the percentage of tags correctly labeled (matching
3 Tags are also applied to punctuation, so tagging assumes tokenizing of commas, quotation marks, etc., and disambiguating end-of-sentence periods from periods inside words (e.g., etc.).
4 Note the large differences across the two genres, especially in token frequency. Tags in the WSJ corpus
are less ambiguous; its focus on financial news leads to a more limited distribution of word usages than the diverse genres of the Brown corpus.
  Types: Unambiguous Ambiguous
(1 tag) (2+tags)
WSJ 44,432 (86%)
7,025 (14%)
Brown 45,799 (85%)
8,050 (15%)
Tokens:
Unambiguous (1tag) Ambiguous (2+tags)
577,421 (45%) 384,349 (33%) 711,780 (55%) 786,646 (67%)
  Figure 8.2
 accuracy
Most Frequent Class Baseline: Always compare a classifier against a baseline at least as good as the most frequent class baseline (assigning each token to the class it occurred in most often in the training set).

8.4 • HMM PART-OF-SPEECH TAGGING 7
human labels on a test set). If we train on the WSJ training corpus and test on sec- tions 22-24 of the same corpus the most-frequent-tag baseline achieves an accuracy of 92.34%. By contrast, the state of the art in part-of-speech tagging on this dataset is around 97% tag accuracy, a performance that is achievable by most algorithms (HMMs, MEMMs, neural networks, rule-based algorithms). See Section 8.7 on other languages and genres.
8.4 HMM Part-of-Speech Tagging
sequence model
Markov chain
In this section we introduce the use of the Hidden Markov Model for part-of-speech tagging. The HMM is a sequence model. A sequence model or sequence classi- fier is a model whose job is to assign a label or class to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels. An HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), it computes a probability distribution over possible sequences of labels and chooses the best label sequence.
8.4.1 Markov Chains
The HMM is based on augmenting the Markov chain. A Markov chain is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no im- pact on the future except via the current state. It’s as if to predict tomorrow’s weather you could examine today’s weather but you weren’t allowed to look at yesterday’s weather.
A Markov chain for weather (a) and one for words (b), showing states and transitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.
More formally, consider a sequence of state variables q1,q2,...,qi. A Markov model embodies the Markov assumption on the probabilities of this sequence: that when predicting the future, the past doesn’t matter, only the present.
Markov Assumption: P(qi = a|q1...qi−1) = P(qi = a|qi−1) (8.4)
Figure 8.3a shows a Markov chain for assigning a probability to a sequence of weather events, for which the vocabulary consists of HOT, COLD, and WARM. The
         .1
HOT1
.1
.3
COLD2
.8
.1 .3
.1
WARM3
     (a) (b)
.4
uniformly
.1
are
.5
.2 .5
   .5 .6
charming
.2
   .6
.6
  Figure 8.3
Markov assumption

8
CHAPTER 8 • PART-OF-SPEECH TAGGING
 Q = q1q2 ...qN a set of N states
 A=a11a12...an1...ann a transition probability matrix A, each aij represent- ing the probability of moving from state i to state j, s.t.
􏰄 nj = 1 a i j = 1 ∀ i
 π = π1,π2,...,πN an initial probability distribution over states. πi is the probability that the Markov chain will start in state i. Some states j may have π j = 0, meaning that they cannot
be initial states. Also, 􏰄ni=1 πi = 1
hidden
Hidden Markov model
states are represented as nodes in the graph, and the transitions, with their probabil- ities, as edges. The transitions are probabilities: the values of arcs leaving a given state must sum to 1. Figure 8.3b shows a Markov chain for assigning a probability to a sequence of words w1...wn. This Markov chain should be familiar; in fact, it repre- sents a bigram language model, with each edge expressing the probability p(wi|wj)! Given the two models in Fig. 8.3, we can assign a probability to any sequence from our vocabulary.
Formally, a Markov chain is specified by the following components:
Before you go on, use the sample probabilities in Fig. 8.3a (with π = [0.1, 0.7, 0.2]) to compute the probability of each of the following sequences:
(8.5) hothothothot (8.6) coldhotcoldhot
What does the difference in these probabilities tell you about a real-world weather fact encoded in Fig. 8.3a?
8.4.2 The Hidden Markov Model
A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don’t observe them directly. For example we don’t normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed.
A hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. An HMM is specified by the following components:
 Q = q1q2 ...qN a set of N states
 A=a11...aij...aNN atransitionprobabilitymatrixA,eachaij representingtheprobability
􏰄N
of moving from state i to state j, s.t. j=1 aij = 1 ∀i
 O = o1o2 ...oT a sequence of T observations, each one drawn from a vocabulary V = v1,v2,...,vV
 B = bi (ot ) a sequence of observation likelihoods, also called emission probabili- ties, each expressing the probability of an observation ot being generated
from a state qi
 π = π1,π2,...,πN an initial probability distribution over states. πi is the probability that the Markov chain will start in state i. Some states j may have π j = 0,
meaning that they cannot be initial states. Also, 􏰄ni=1 πi = 1

8.4 • HMM PART-OF-SPEECH TAGGING 9
A first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state:
Markov Assumption: P(qi|q1...qi−1) = P(qi|qi−1) (8.7) Second, the probability of an output observation oi depends only on the state that
produced the observation qi and not on any other states or any other observations: Output Independence: P(oi|q1 ...qi,...,qT ,o1,...,oi,...,oT ) = P(oi|qi) (8.8)
8.4.3 The components of an HMM tagger
Let’s start by looking at the pieces of an HMM tagger, and then we’ll see how to use it to tag. An HMM has two components, the A and B probabilities.
The A matrix contains the tag transition probabilities P(ti|ti−1) which represent the probability of a tag occurring given the previous tag. For example, modal verbs like will are very likely to be followed by a verb in the base form, a VB, like race, so we expect this probability to be high. We compute the maximum likelihood estimate of this transition probability by counting, out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second:
P(ti|ti−1) = C(ti−1,ti) (8.9) C(ti−1)
In the WSJ corpus, for example, MD occurs 13124 times of which it is followed by VB 10471, for an MLE estimate of
P(V B|MD) = C(MD,V B) = 10471 = .80 (8.10) C(MD) 13124
Let’s walk through an example, seeing how these probabilities are estimated and used in a sample tagging task, before we return to the algorithm for decoding.
In HMM tagging, the probabilities are estimated by counting on a tagged training corpus. For this example we’ll use the tagged WSJ corpus.
The B emission probabilities, P(wi|ti), represent the probability, given a tag (say MD), that it will be associated with a given word (say will). The MLE of the emis- sion probability is
P(wi|ti) = C(ti,wi) (8.11) C(ti)
Of the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046 times:
P(will|MD) = C(MD,will) = 4046 = .31 (8.12) C(MD) 13124
We saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood term is not asking “which is the most likely tag for the word will?” That would be the posterior P(MD|will). Instead, P(will|MD) answers the slightly counterintuitive question “If we were going to generate a MD, how likely is it that this modal would be will?”
The A transition probabilities, and B observation likelihoods of the HMM are illustrated in Fig. 8.4 for three states in an HMM part-of-speech tagger; the full tagger would have one state for each tag.

10 CHAPTER 8 • PART-OF-SPEECH TAGGING
     B2
P("aardvark" | MD)
...
P(“will” | MD)
...
P("the" | MD)
...
P(“back” | MD) ...
P("zebra" | MD)
     a12 a11
VB1
a22
MD2
a21 a a13
a
a32
23
31
NN3
a33
B3
P("aardvark" | NN) ...
P(“will” | NN)
...
P("the" | NN) ...
P(“back” | NN) ...
P("zebra" | NN)
   B1
P("aardvark" | VB)
...
P(“will” | VB)
...
P("the" | VB) ...
P(“back” | VB) ...
P("zebra" | VB)
       Figure 8.4
decoding
An illustration of the two parts of an HMM representation: the A transition probabilities used to compute the prior probability, and the B observation likelihoods that are associated with each state, one likelihood for each possible observation word.
8.4.4 HMM tagging as decoding
For any model, such as an HMM, that contains hidden variables, the task of deter- mining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally,
Decoding: Given as input an HMM λ = (A, B) and a sequence of ob- servations O = o1 , o2 , ..., oT , find the most probable sequence of states Q = q1q2q3 ...qT .
For part-of-speech tagging, the goal of HMM decoding is to choose the tag se- quence t1n that is most probable given the observation sequence of n words wn1:
nnn tˆ =argmaxP(t |w ) 111
t1n
The way we’ll do this in the HMM is to use Bayes’ rule to instead compute:
(8.13)
(8.14)
(8.15)
n
tˆ =argmax
P(wn1 |t1n )P(t1n ) P ( w n1 )
 1 t 1n
Furthermore, we simplify Eq. 8.14 by dropping the denominator P(wn1):
nnnn tˆ =argmaxP(w |t )P(t ) 1111
t1n
HMM taggers make two further simplifying assumptions. The first is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags:
n
P(wn1|t1n) ≈ 􏰆P(wi|ti) (8.16)
i=1
The second assumption, the bigram assumption, is that the probability of a tag is
dependent only on the previous tag, rather than the entire tag sequence;
n
P(t1n) ≈ 􏰆P(ti|ti−1) (8.17)
i=1

Viterbi algorithm
The two parts of Eq. 8.18 correspond neatly to the B emission probability and A transition probability that we just defined above!
8.4.5 The Viterbi Algorithm
The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.5. As an instance of dynamic programming, Viterbi resembles the dynamic program- ming minimum edit distance algorithm of Chapter 2.
8.4 • HMM PART-OF-SPEECH TAGGING 11
Plugging the simplifying assumptions from Eq. 8.16 and Eq. 8.17 into Eq. 8.15 results in the following equation for the most probable tag sequence from a bigram tagger:
n emission transition 􏰆 􏰇 􏰊􏰉 􏰈 􏰇 􏰊􏰉 􏰈
nnn
tˆ = argmaxP(t |w ) ≈ argmax P(w |t ) P(t |t ) (8.18)
1 11 iiii−1 t1n t1n i=1
   function VITERBI(observations of len T,state-graph of len N) returns best-path, path-prob
create a path probability matrix viterbi[N,T] for each state s from 1 to N do
viterbi[s,1]←πs ∗ bs(o1)
backpointer[s,1] ← 0
for each time step t from 2 to T do
for each state s from 1 to N do N′
; initialization step
; recursion step viterbi[s,t] ← max viterbi[s , t − 1] ∗ as′ ,s ∗ bs (ot )
s′ =1
backpointer[s,t] ← argmax viterbi[s , t − 1] ∗ as′ ,s ∗ bs (ot )
N bestpathprob←max viterbi[s,T]
s=1
N bestpathpointer←argmax viterbi[s,T]
;terminationstep ;terminationstep
N′ s′ =1
s=1
bestpath ← the path starting at state bestpathpointer, that follows backpointer[] to states back in time
return bestpath, bestpathprob
  Figure 8.5
Viterbi algorithm for finding the optimal sequence of tags. Given an observation sequence and an HMM λ = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood to the observation sequence.
The Viterbi algorithm first sets up a probability matrix or lattice, with one col- umn for each observation ot and one row for each state in the state graph. Each col- umn thus has a cell for each state qi in the single combined automaton. Figure 8.6 shows an intuition of this lattice for the sentence Janet will back the bill.
Each cell of the lattice, vt ( j), represents the probability that the HMM is in state j after seeing the first t observations and passing through the most probable state sequence q1,...,qt−1, given the HMM λ. The value of each cell vt(j) is computed by recursively taking the most probable path that could lead us to this cell. Formally,
each cell expresses the probability
vt(j) = max P(q1...qt−1,o1,o2 ...ot,qt = j|λ) (8.19)
q1 ,...,qt −1
We represent the most probable path by taking the maximum over all possible
previous state sequences max . Like other dynamic programming algorithms, q1 ,...,qt −1

12 CHAPTER 8 • PART-OF-SPEECH TAGGING
A sketch of the lattice for Janet will back the bill, showing the possible tags (qi) for each word and highlighting the path corresponding to the correct tag sequence through the hidden states. States (parts of speech) which have a zero probability of generating a particular word according to the B matrix (such as the probability that a determiner DT will be realized as Janet) are greyed out.
Viterbi fills each cell recursively. Given that we had already computed the probabil- ity of being in every state at time t − 1, we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. For a given state qj at time t, the value vt(j) is computed as
N
vt(j) = maxvt−1(i) aij bj(ot) (8.20)
i=1
The three factors that are multiplied in Eq. 8.20 for extending the previous paths to
compute the Viterbi probability at time t are
8.4.6 Working through an example
Let’s tag the sentence Janet will back the bill; the goal is the correct series of tags (see also Fig. 8.6):
(8.21) Janet/NNP will/MD back/VB the/DT bill/NN
Let the HMM be defined by the two tables in Fig. 8.7 and Fig. 8.8. Figure 8.7 lists the ai j probabilities for transitioning between the hidden states (part-of-speech tags). Figure 8.8 expresses the bi(ot) probabilities, the observation likelihoods of words given tags. This table is (slightly simplified) from counts in the WSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts of speech, and the word the can appear as a determiner or as an NNP (in titles like “Somewhere Over the Rainbow” all words are tagged as NNP).
Figure 8.9 shows a fleshed-out version of the sketch we saw in Fig. 8.6, the Viterbi lattice for computing the best hidden state sequence for the observation se- quence Janet will back the bill.
        DT DT DT DT DT RB RB RB RB RB NN NN NN NN NN JJ JJ JJ JJ JJ VB VB VB VB VB MD MD MD MD MD
NNP NNP NNP NNP NNP Janet will back the bill
                                     Figure 8.6
 vt−1(i) the previous Viterbi path probability from the previous time step
 ai j the transition probability from previous state qi to current state q j
 bj(ot) the state observation likelihood of the observation symbol ot given the current state j

8.4 • HMM PART-OF-SPEECH TAGGING 13
  NNP MD VB JJ NN RB DT
 <s> 0.2767 0.0006 0.0031 0.0453 0.0449 0.0510 0.2026 NNP 0.3777 0.0110 0.0009 0.0084 0.0584 0.0090 0.0025 MD 0.0008 0.0002 0.7968 0.0005 0.0008 0.1698 0.0041 VB 0.0322 0.0005 0.0050 0.0837 0.0615 0.0514 0.2231 JJ 0.0366 0.0004 0.0001 0.0733 0.4509 0.0036 0.0036 NN 0.0096 0.0176 0.0014 0.0086 0.1216 0.0177 0.0068 RB 0.0068 0.0102 0.1011 0.1012 0.0120 0.0728 0.0479 DT 0.1147 0.0021 0.0002 0.2157 0.4744 0.0102 0.0017
  Figure 8.7
The A transition probabilities P(ti|ti−1) computed from the WSJ corpus without smoothing. Rows are labeled with the conditioning event; thus P(V B|MD) is 0.7968.
Observation likelihoods B computed from the WSJ corpus without smoothing, simplified slightly.
There are N = 5 state columns. We begin in column 1 (for the word Janet) by setting the Viterbi value in each cell to the product of the π transition probability (the start probability for that state i, which we get from the <s > entry of Fig. 8.7), and the observation likelihood of the word Janet given the tag for that cell. Most of the cells in the column are zero since the word Janet cannot be any of those tags. The reader should find this in Fig. 8.9.
Next, each cell in the will column gets updated. For each state, we compute the value viterbi[s,t] by taking the maximum over the extensions of all the paths from the previous column that lead to the current cell according to Eq. 8.20. We have shown the values for the MD, VB, and NN cells. Each cell gets the max of the 7 values from the previous column, multiplied by the appropriate transition probabil- ity; as it happens in this case, most of them are zero from the previous column. The remaining value is multiplied by the relevant observation probability, and the (triv- ial) max is taken. In this case the final value, 2.772e-8, comes from the NNP state at the previous column. The reader should fill in the rest of the lattice in Fig. 8.9 and backtrace to see whether or not the Viterbi algorithm returns the gold state sequence NNP MD VB DT NN.
8.4.7 Extending the HMM Algorithm to Trigrams
Practical HMM taggers have a number of extensions of this simple model. One important missing feature is a wider tag context. In the tagger described above the probability of a tag depends only on the previous tag:
n
P(t1n) ≈ 􏰆P(ti|ti−1) (8.22)
  Janet will back the bill
 NNP 0.000032 0
MD 0 0.308431 VB 0 0.000028 JJ 0 0
NN 0 0.000200 RB 0 0
DT 0 0
0
0 0.000672 0.000340 0.000223 0.010446 0
0.000048 0
0 0
0 0.000028 0 0
0 0.002337 0 0 0.506099 0
  Figure 8.8
i=1

14 CHAPTER 8 • PART-OF-SPEECH TAGGING
The first few entries in the individual state columns for the Viterbi algorithm. Each cell keeps the probability of the best path so far and a pointer to the previous cell along that path. We have only filled out columns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the reader. After the cells are filled in, backtracing from the end state, we should be able to reconstruct the correct state sequence NNP MD VB DT NN.
In practice we use more of the history, letting the probability of a tag depend on the two previous tags:
n
P(t1n) ≈ 􏰆P(ti|ti−1,ti−2) (8.23)
i=1
Extending the algorithm from bigram to trigram taggers gives a small (perhaps a half point) increase in performance, but conditioning on two previous tags instead of one requires a significant change to the Viterbi algorithm. For each cell, instead of taking a max over transitions from each cell in the previous column, we have to take a max over paths through the cells in the previous two columns, thus considering N2 rather than N hidden states at every observation.
In addition to increasing the context window, HMM taggers have a number of other advanced features. One is to let the tagger know the location of the end of the sentence by adding dependence on an end-of-sequence marker for tn+1. This gives the following equation for part-of-speech tagging:
􏰍n􏰎
􏰆
t1n i=1
             q7 DT
v1(7)
v1(6)
v1(5)
v1(4)= . 045*0=0
v1(3)= .0031 x 0
=0
v1(2)= .0006 x 0 =
0
v1(1) = .28*.000032
= .000009
start
Janet
v2(7)
v2(6) v2(5)=
max * .0002 = .0000000001
v2(4) v2(3)=
max * .000028 = 2.5e-13
v2(2) =
max*.308= 2.772e-8
v2(1)
start will
t
           q
q5 NN q4 JJ
q3 VB q2 MD
q1 NNP π
v3(6)= max * .0104
v3(5)= max * .
000223
v3(4)= max * .00034
v3(3)= max * .00067
start start start
6 RB
           * P(NN|NN)
                                                   backtrace backtrace
       back the bill o1 o2 o3 o4 o5
   Figure 8.9
nnn
tˆ =argmaxP(t |w )≈argmax
P(w|t)P(t|t ,t ) P(t |t ) (8.24) iiii−1i−2 n+1n
1 11 t1n
* P(RB|NN)
P(JJ |start) = .045
* P(MD|JJ) =0
P(VB|start) = .0031
* P(MD|VB) =0
P(MD|start) = .0006
* P(MD|MD) =0
P(NNP|start) = .28
* P(MD|NNP) .000009*.01 = .9e-8

8.4 • HMM PART-OF-SPEECH TAGGING 15
In tagging any sentence with Eq. 8.24, three of the tags used in the context will fall off the edge of the sentence, and hence will not match regular words. These tags, t−1, t0, and tn+1, can all be set to be a single special ‘sentence boundary’ tag that is added to the tagset, which assumes sentences boundaries have already been marked.
One problem with trigram taggers as instantiated in Eq. 8.24 is data sparsity. Any particular sequence of tags ti−2,ti−1,ti that occurs in the test set may simply never have occurred in the training set. That means we cannot compute the tag trigram probability just by the maximum likelihood estimate from counts, following Eq. 8.25:
P(ti|ti−1,ti−2) = C(ti−2,ti−1,ti) (8.25) C(ti−2,ti−1)
Just as we saw with language modeling, many of these counts will be zero in any training set, and we will incorrectly predict that a given tag sequence will never occur! What we need is a way to estimate P(ti|ti−1,ti−2) even if the sequence ti−2,ti−1,ti never occurs in the training data.
The standard approach to solving this problem is the same interpolation idea we saw in language modeling: estimate the probability by combining more robust, but weaker estimators. For example, if we’ve never seen the tag sequence PRP VB TO, and so can’t compute P(TO|PRP,VB) from this frequency, we still could rely on the bigram probability P(TO|VB), or even the unigram probability P(TO). The maximum likelihood estimation of each of these probabilities can be computed from a corpus with the following counts:
 Trigrams Pˆ(ti|ti−1,ti−2) = C(ti−2,ti−1,ti) C(ti−2,ti−1)
Bigrams Pˆ(ti|ti−1) = C(ti−1,ti) C(ti−1)
Unigrams Pˆ(ti) = C(ti) N
(8.26)
(8.27)
(8.28)
  deleted interpolation
The standard way to combine these three estimators to estimate the trigram probabil- ity P(ti|ti−1,ti−2) is via linear interpolation. We estimate the probability P(ti|ti−1ti−2) by a weighted sum of the unigram, bigram, and trigram probabilities:
ˆˆˆ
P(ti|ti−1ti−2) = λ3P(ti|ti−1ti−2)+λ2P(ti|ti−1)+λ1P(ti) (8.29)
We require λ1 + λ2 + λ3 = 1, ensuring that the resulting P is a probability distri- bution. The λs are set by deleted interpolation (Jelinek and Mercer, 1980): we successively delete each trigram from the training corpus and choose the λ s so as to maximize the likelihood of the rest of the corpus. The deletion helps to set the λs in such a way as to generalize to unseen data and not overfit. Figure 8.10 gives a deleted interpolation algorithm for tag trigrams.
8.4.8 Beam Search
When the number of states grows very large, the vanilla Viterbi algorithm is slow. The complexity of the algorithm is O(N2T); N (the number of states) can be large for trigram taggers, which have to consider every previous pair of the 45 tags, re- sulting in 453 = 91, 125 computations per column. N can be even larger for other applications of Viterbi, for example to decoding in neural networks, as we will see in future chapters.

16
CHAPTER 8 • PART-OF-SPEECH TAGGING
   function DELETED-INTERPOLATION(corpus) returns λ1,λ2,λ3
λ1, λ2, λ3←0
foreach trigram t1,t2,t3 with C(t1,t2,t3) > 0
depending on the maximum of the following three values case C(t1,t2,t3)−1 : increment λ3 by C(t1,t2,t3)
 C(t1 ,t2 )−1
case C(t2,t3)−1 : increment λ2 by C(t1,t2,t3)
C(t2)−1
case C(t3)−1 : increment λ1 by C(t1,t2,t3)
N−1 end
end
normalize λ1,λ2,λ3 return λ1,λ2,λ3
  Figure 8.10
beam search
beam width
The deleted interpolation algorithm for setting the weights for combining uni- gram, bigram, and trigram tag probabilities. If the denominator is 0 for any case, we define the result of that case to be 0. N is the number of tokens in the corpus. After Brants (2000).
One common solution to the complexity problem is the use of beam search decoding. In beam search, instead of keeping the entire column of states at each time point t, we just keep the best few hypothesis at that point. At time t this requires computing the Viterbi score for each of the N cells, sorting the scores, and keeping only the best-scoring states. The rest are pruned out and not continued forward to timet+1.
One way to implement beam search is to keep a fixed number of states instead of all N current states. Here the beam width β is a fixed number of states. Alternatively β can be modeled as a fixed percentage of the N states, or as a probability threshold. Figure 8.11 shows the search lattice using a beam width of 2 states.
A beam search version of Fig. 8.6, showing a beam width of 2. At each time t, all (non-zero) states are computed, but then they are sorted and only the best 2 states are propagated forward and the rest are pruned, shown in orange.
        DT DT DT DT DT RB RB RB RB RB NN NN NN NN NN JJ JJ JJ JJ JJ VB VB VB VB VB MD MD MD MD MD
NNP NNP NNP NNP NNP Janet will back the bill
                                     Figure 8.11

8.5
Maximum Entropy Markov Models
While an HMM can achieve very high accuracy, we saw that it requires a number of architectural innovations to deal with unknown words, backoff, suffixes, and so on. It would be so much easier if we could add arbitrary features directly into the model in a clean way, but that’s hard for generative models like HMMs. Luckily, we’ve already seen a model for doing this: the logistic regression model of Chapter 5! But logistic regression isn’t a sequence model; it assigns a class to a single observation. However, we could turn logistic regression into a discriminative sequence model simply by running it on successive words, using the class assigned to the prior word
unknown words
To achieve high accuracy with part-of-speech taggers, it is also important to have a good model for dealing with unknown words. Proper names and acronyms are created very often, and even new common nouns and verbs enter the language at a surprising rate. One useful feature for distinguishing parts of speech is word shape: words starting with capital letters are likely to be proper nouns (NNP).
But the strongest source of information for guessing the part-of-speech of un- known words is morphology. Words that end in -s are likely to be plural nouns (NNS), words ending with -ed tend to be past participles (VBN), words ending with -able adjectives (JJ), and so on. We store for each final letter sequence (for sim- plicity referred to as word suffixes) of up to 10 letters the statistics of the tag it was associated with in training. We are thus computing for each suffix of length i the probability of the tag ti given the suffix letters (Samuelsson 1993, Brants 2000):
P(ti|ln−i+1 ...ln) (8.30)
Back-off is used to smooth these probabilities with successively shorter suffixes. Because unknown words are unlikely to be closed-class words like prepositions, suffix probabilities can be computed only for words whose training set frequency is ≤ 10, or only for open-class words. Separate suffix tries are kept for capitalized and uncapitalized words.
Finally, because Eq. 8.30 gives a posterior estimate p(ti|wi), we can compute the likelihood p(wi|ti) that HMMs require by using Bayesian inversion (i.e., using Bayes’ rule and computation of the two priors P(ti) and P(ti|ln−i+1 ...ln)).
In addition to using capitalization information for unknown words, Brants (2000) also uses capitalization for known words by adding a capitalization feature to each tag. Thus, instead of computing P(ti|ti−1,ti−2) as in Eq. 8.26, the algorithm com- putes the probability P(ti,ci|ti−1,ci−1,ti−2,ci−2). This is equivalent to having a cap- italized and uncapitalized version of each tag, doubling the size of the tagset.
Combining all these features, a trigram HMM like that of Brants (2000) has a tagging accuracy of 96.7% on the Penn Treebank, perhaps just slightly below the performance of the best MEMM and neural taggers.
8.5 • MAXIMUM ENTROPY MARKOV MODELS 17 8.4.9 Unknown Words
words people never use — could be only I
know them
Ishikawa Takuboku 1885–1912

18 CHAPTER 8 • PART-OF-SPEECH TAGGING
as a feature in the classification of the next word. When we apply logistic regression
5
MEMM in this way, it’s called the maximum entropy Markov model or MEMM. nn
Let the sequence of words be W = w1 and the sequence of tags T = t1. In an HMM to compute the best tag sequence that maximizes P(T |W ) we rely on Bayes’ rule and the likelihood P(W |T ):
ˆ
T = argmax P(T |W )
T
= argmaxP(W|T)P(T) T
= argmax􏰆P(wordi|tagi)􏰆P(tagi|tagi−1)
T
In an MEMM, by contrast, we compute the posterior P(T |W ) directly, training it to
discriminate among the possible tag sequences: ˆ
= argmax􏰆P(ti|wi,ti−1)
ii
T = argmax P(T |W ) T
T
i
Consider tagging just one word. A multinomial logistic regression classifier could compute the single probability P(ti|wi,ti−1) in a different way than an HMM. Fig. 8.12 shows the intuition of the difference via the direction of the arrows; HMMs compute likelihood (observation word conditioned on tags) but MEMMs compute posterior (tags conditioned on observation words).
A schematic view of the HMM (top) and MEMM (bottom) representation of the probability computation for the correct sequence of tags for the back sentence. The HMM computes the likelihood of the observation given the hidden state, while the MEMM computes the posterior of each state, conditioned on the previous state and current observation.
8.5.1 Features in a MEMM
Of course we don’t build MEMMs that condition just on wi and ti−1. The reason to use a discriminative sequence model is that it’s easier to incorporate a lot of fea-
6
(8.31)
(8.32)
       NNP MD VB DT NN
Janet will back the bill
       NNP MD VB DT NN
Janet will back the bill
  Figure 8.12
  tures.
Figure 8.13 shows a graphical intuition of some of these additional features.
 5 ‘Maximum entropy model’ is an outdated name for logistic regression; see the history section.
6 Because in HMMs all computation is based on the two probabilities P(tag|tag) and P(word|tag), if
we want to include some source of knowledge into the tagging process, we must find a way to encode the knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of complicated conditioning which gets harder and harder as we have more and more such features.

templates
A basic MEMM part-of-speech tagger conditions on the observation word it- self, neighboring words, and previous tags, and various combinations, using feature templates like the following:
⟨ti,wi−2⟩,⟨ti,wi−1⟩,⟨ti,wi⟩,⟨ti,wi+1⟩,⟨ti,wi+2⟩ ⟨ti,ti−1⟩,⟨ti,ti−2,ti−1⟩,
⟨ti,ti−1,wi⟩,⟨ti,wi−1,wi⟩⟨ti,wi,wi+1⟩, (8.33)
Recall from Chapter 5 that feature templates are used to automatically populate the set of features from every instance in the training and test set. Thus our example Janet/NNP will/MD back/VB the/DT bill/NN, when wi is the word back, would gen- erate the following features:
ti = VB and wi−2 = Janet ti = VB and wi−1 = will ti =VBandwi =back
ti =VBandwi+1 =the
ti = VB and wi+2 = bill
ti =VBandti−1 =MD
ti =VBandti−1 =MDandti−2 =NNP ti =VBandwi =backandwi+1 =the
Also necessary are features to deal with unknown words, expressing properties of the word’s spelling or shape:
8.5 • MAXIMUM ENTROPY MARKOV MODELS 19
An MEMM for part-of-speech tagging showing the ability to condition on
    ti-2 ti-1
NNP MD VB
wi-2 wi-1 wi wi+1
<s> Janet will back the bill
       Figure 8.13
more features.
 wi contains a particular prefix (from all prefixes of length ≤ 4)
 wi contains a particular suffix (from all suffixes of length ≤ 4) wi contains a number
wi contains an upper-case letter
wi contains a hyphen
wi is all upper case
 wi’s word shape
wi’s short word shape
wi is upper case and has a digit and a dash (like CFC-12)
wi is upper case and followed within 3 words by Co., Inc., etc.
word shape
Word shape features are used to represent the abstract letter pattern of the word by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to ’d’, and retaining punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30 would map to XXdd-dd. A second class of shorter word shape features is also used. In these features consecutive character types are removed, so DC10-30 would be mapped to Xd-d but I.M.F would still map to X.X.X. For example the word well-dressed would generate the following non-zero valued feature values:

20 CHAPTER 8 •
PART-OF-SPEECH TAGGING
prefix(wi) = w
prefix(wi) = we
prefix(wi) = wel
prefix(wi) = well
suffix(wi) = ssed
suffix(wi) = sed
suffix(wi) = ed
suffix(wi) = d
has-hyphen(wi )
word-shape(wi) = xxxx-xxxxxxx short-word-shape(wi) = x-x
Features for known words, like the templates in Eq. 8.33, are computed for every word seen in the training set. The unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set.
8.5.2 Decoding and Training MEMMs
The most likely sequence of tags is then computed by combining these features of the input word wi, its neighbors within l words wi+l, and the previous k tags ti−1 as
i−l i−k follows (using θ to refer to feature weights instead of w to avoid the confusion with
w meaning words): ˆ
= argmax􏰆P(ti|wi+l,ti−1)
T = argmax P(T |W ) T
T
= argmax T
i
i−l i−k 
i
􏰆
exp
􏰅
t ′ ∈tagset
􏰅θj fj(ti,wi+l,ti−1) i−l i−k 
j
exp

 greedy
ˆ
How should we decode to find this optimal tag sequence T? The simplest way
to turn logistic regression into a sequence model is to build a local classifier that classifies each word left to right, making a hard classification on the first word in the sentence, then a hard decision on the second word, and so on. This is called a greedy decoding algorithm, because we greedily choose the best tag for each word, as shown in Fig. 8.14.
In greedy decoding we simply run the classifier on each token, left to right, each time making a hard decision about which is the best tag.
  􏰅θj fj(t′,wi+l,ti−1)
(8.34)
 i−l i−k  j
   function GREEDY SEQUENCE DECODING(words W, model P) returns tag sequence T for i = 1 to length(W)
tˆ =argmaxP(t′|wi+l,ti−1)
i
t′∈ T
i−l i−k
  Figure 8.14

Viterbi
8.6 • BIDIRECTIONALITY 21
The problem with the greedy algorithm is that by making a hard decision on each word before moving on to the next word, the classifier can’t use evidence from future decisions. Although the greedy algorithm is very fast, and occasionally has sufficient accuracy to be useful, in general the hard decision causes too great a drop in performance, and we don’t use it.
Instead we decode an MEMM with the Viterbi algorithm just as with the HMM, finding the sequence of part-of-speech tags that is optimal for the whole sentence.
For example, assume that our MEMM is only conditioning on the previous tag ti−1 and observed word wi. Concretely, this involves filling an N × T array with the appropriate values for P(ti|ti−1,wi), maintaining backpointers as we proceed. As with HMM Viterbi, when the table is filled, we simply follow pointers back from the maximum value in the final column to retrieve the desired set of labels. The requisite changes from the HMM-style application of Viterbi have to do only with how we fill each cell. Recall from Eq. 8.20 that the recursive step of the Viterbi equation computes the Viterbi value of time t for state j as
N
vt(j) = max vt−1(i)aijbj(ot); 1≤ j≤N,1<t≤T
i=1
which is the HMM implementation of
N
vt(j) = max vt−1(i)P(sj|si)P(ot|sj) 1≤ j≤N,1<t≤T
(8.35)
(8.36)
i=1
The MEMM requires only a slight change to this latter formula, replacing the a and
b prior and likelihood probabilities with the direct posterior:
N
vt(j) = max vt−1(i)P(sj|si,ot) 1≤j≤N,1<t≤T (8.37)
i=1
Learning in MEMMs relies on the same supervised learning algorithms we presented for logistic regression. Given a sequence of observations, feature functions, and cor- responding hidden states, we use gradient descent to train the weights to maximize the log-likelihood of the training corpus.
8.6 Bidirectionality
label bias
observation bias
The one problem with the MEMM and HMM models as presented is that they are exclusively run left-to-right. While the Viterbi algorithm still allows present deci- sions to be influenced indirectly by future decisions, it would help even more if a decision about word wi could directly use information about future tags ti+1 and ti+2.
Adding bidirectionality has another useful advantage. MEMMs have a theoret- ical weakness, referred to alternatively as the label bias or observation bias prob- lem (Lafferty et al. 2001, Toutanova et al. 2003). These are names for situations when one source of information is ignored because it is explained away by another source. Consider an example from Toutanova et al. (2003), the sequence will/NN to/TO fight/VB. The tag TO is often preceded by NN but rarely by modals (MD), and so that tendency should help predict the correct NN tag for will. But the previ- ous transition P(twill|⟨s⟩) prefers the modal, and because P(TO|to,twill) is so close to 1 regardless of twill the model cannot make use of the transition probability and incorrectly chooses MD. The strong information that to must have the tag TO has ex- plained away the presence of TO and so the model doesn’t learn the importance of

22 CHAPTER 8 • PART-OF-SPEECH TAGGING
CRF
Stanford tagger
the previous NN tag for predicting TO. Bidirectionality helps the model by making the link between TO available when tagging the NN.
One way to implement bidirectionality is to switch to a more powerful model called a conditional random field or CRF. The CRF is an undirected graphical model, which means that it’s not computing a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a clique, a set of relevant features. Unlike for an MEMM, these might include output features of words in future time steps. The probability of the best sequence is similarly computed by the Viterbi algorithm. Because a CRF normalizes probabilities over all tag sequences, rather than over all the tags at an individual time t, training requires computing the sum over all possible labelings, which makes CRF training quite slow.
Simpler methods can also be used; the Stanford tagger uses a bidirectional version of the MEMM called a cyclic dependency network (Toutanova et al., 2003). Alternatively, any sequence model can be turned into a bidirectional model by using multiple passes. For example, the first pass would use only part-of-speech features from already-disambiguated words on the left. In the second pass, tags for all words, including those on the right, can be used. Alternately, the tagger can be run twice, once left-to-right and once right-to-left. In greedy decoding, for each word the classifier chooses the highest-scoring of the tags assigned by the left-to-right and right-to-left classifier. In Viterbi decoding, the classifier chooses the higher scoring of the two sequences (left-to-right or right-to-left). These bidirectional models lead directly into the bi-LSTM models that we will introduce in Chapter 9 as a standard
neural sequence model.
8.7 Part-of-Speech Tagging for Morphological Rich Lan- guages
Augmentations to tagging algorithms become necessary when dealing with lan- guages with rich morphology like Czech, Hungarian and Turkish.
These productive word-formation processes result in a large vocabulary for these languages: a 250,000 word token corpus of Hungarian has more than twice as many word types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while a 10 million word token corpus of Turkish contains four times as many word types as a similarly sized English corpus (Hakkani-Tu ̈r et al., 2002). Large vocabular- ies mean many unknown words, and these unknown words cause significant per- formance degradations in a wide variety of languages (including Czech, Slovene, Estonian, and Romanian) (Hajicˇ, 2000).
Highly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine). Because this information is important for tasks like pars- ing and coreference resolution, part-of-speech taggers for morphologically rich lan- guages need to label words with case and gender information. Tagsets for morpho- logically rich languages are therefore sequences of morphological tags rather than a single primitive tag. Here’s a Turkish example, in which the word izin has three pos- sible morphological/part-of-speech tags and meanings (Hakkani-Tu ̈r et al., 2002):
1. Yerdekiizintemizlenmesigerek. iz+Noun+A3sg+Pnon+Gen The trace on the floor should be cleaned.
 ̈
2. Uzerindeparmakizinkalmis ̧ iz+Noun+A3sg+P2sg+Nom

8.8 • SUMMARY 23
Your finger print is left on (it).
3. Ic ̧erigirmekic ̧inizinalmangerekiyor. izin+Noun+A3sg+Pnon+Nom
You need permission to enter.
Using a morphological parse sequence like Noun+A3sg+Pnon+Gen as the part- of-speech tag greatly increases the number of parts of speech, and so tagsets can be 4 to 10 times larger than the 50–100 tags we have seen for English. With such large tagsets, each word needs to be morphologically analyzed to generate the list of possible morphological tag sequences (part-of-speech tags) for the word. The role of the tagger is then to disambiguate among these tags. This method also helps with unknown words since morphological parsers can accept unknown stems and still segment the affixes properly.
For non-word-space languages like Chinese, word segmentation (Chapter 2) is either applied before tagging or done jointly. Although Chinese words are on aver- age very short (around 2.4 characters per unknown word compared with 7.7 for En- glish) the problem of unknown words is still large. While English unknown words tend to be proper nouns in Chinese the majority of unknown words are common nouns and verbs because of extensive compounding. Tagging models for Chinese use similar unknown word features to English, including character prefix and suf- fix features, as well as novel features like the radicals of each character in a word. (Tseng et al., 2005).
A standard for multilingual tagging is the Universal POS tag set of the Universal Dependencies project, which contains 16 tags plus a wide variety of features that can be added to them to create a large tagset for any language (Nivre et al., 2016).
8.8 Summary
This chapter introduced parts of speech and part-of-speech tagging:
• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.
• Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.
• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, MEMM tagging. We will see a third, discriminative neural approach in Chapter 9.
• The probabilities in HMM taggers are estimated by maximum likelihood es- timation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence
• Beam search is a variant of Viterbi decoding that maintains only a fraction of high scoring states rather than all states during decoding.
• Maximum entropy Markov model or MEMM taggers train logistic regres- sion models to pick the best tag given an observation word and its context and the previous tags, and then use Viterbi to choose the best sequence of tags.
• Modern taggers are generally run bidirectionally.

24 CHAPTER 8 • PART-OF-SPEECH TAGGING Bibliographical and Historical Notes
What is probably the earliest part-of-speech tagger was part of the parser in Zellig Harris’s Transformations and Discourse Analysis Project (TDAP), implemented be- tween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962), although earlier systems had used part-of-speech dictionaries. TDAP used 14 hand- written rules for part-of-speech disambiguation; the use of part-of-speech tag se- quences and the relative frequency of tags for a word prefigures all modern algo- rithms. The parser was implemented essentially as a cascade of finite-state trans- ducers; see Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation.
The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had three components: a lexicon, a morphological analyzer, and a context disambigua- tor. The small 1500-word lexicon listed only function words and other irregular words. The morphological analyzer used inflectional and derivational suffixes to as- sign part-of-speech classes. These were run over words to produce candidate parts of speech which were then disambiguated by a set of 500 context rules by relying on surrounding islands of unambiguous words. For example, one rule said that between an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN- ADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used the same architecture as Klein and Simmons (1963), with a bigger dictionary and more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis and Kucˇera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the Brown corpus was then tagged by hand. All these early algorithms were based on a two-stage architecture in which a dictionary was first used to assign each word a set of potential parts of speech, and then lists of handwritten disambiguation rules winnowed the set down to a single part of speech per word.
Soon afterwards probabilistic architectures began to be developed. Probabili- ties were used in tagging by Stolz et al. (1965) and a complete probabilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The Lancaster- Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown corpus, was tagged in the early 1980’s with the CLAWS tagger (Marshall 1983; Marshall 1987; Garside 1987), a probabilistic algorithm that approximated a simplified HMM tag- ger. The algorithm used tag bigram probabilities, but instead of storing the word likelihood of each tag, the algorithm marked tags either as rare (P(tag|word) < .01) infrequent (P(tag|word) < .10) or normally frequent (P(tag|word) > .10).
DeRose (1988) developed a quasi-HMM algorithm, including the use of dy- namic programming, although computing P(t|w)P(w) instead of P(w|t)P(w). The same year, the probabilistic PARTS tagger of Church (1988), (1989) was probably the first implemented HMM tagger, described correctly in Church (1989), although Church (1988) also described the computation incorrectly as P(t|w)P(w) instead of P(w|t)P(w). Church (p.c.) explained that he had simplified for pedagogical pur- poses because using the probability P(t|w) made the idea seem more understandable as “storing a lexicon in an almost standard form”.
Later taggers explicitly introduced the use of the hidden Markov model (Ku- piec 1992; Weischedel et al. 1993; Schu ̈tze and Singer 1994). Merialdo (1994) showed that fully unsupervised EM didn’t work well for the tagging task and that reliance on hand-labeled data was important. Charniak et al. (1993) showed the im- portance of the most frequent tag baseline; the 92.3% number we give above was from Abney et al. (1999). See Brants (2000) for many implementation details of an HMM tagger whose performance is still roughly close to state of the art taggers.

EXERCISES 25
Ratnaparkhi (1996) introduced the MEMM tagger, called MXPOST, and the modern formulation is very much based on his work.
The idea of using letter suffixes for unknown words is quite old; the early Klein and Simmons (1963) system checked all final letter suffixes of lengths 1-5. The probabilistic formulation we described for HMMs comes from Samuelsson (1993). The unknown word features described on page 19 come mainly from (Ratnaparkhi, 1996), with augmentations from Toutanova et al. (2003) and Manning (2011).
State of the art taggers use neural algorithms like the sequence models in Chap- ter 9 or (bidirectional) log-linear models Toutanova et al. (2003). HMM (Brants 2000; Thede and Harper 1999) and MEMM tagger accuracies are likely just a tad lower.
An alternative modern formalism, the English Constraint Grammar systems (Karls- son et al. 1995; Voutilainen 1995; Voutilainen 1999), uses a two-stage formalism much like the early taggers from the 1950s and 1960s. A morphological analyzer with tens of thousands of English word stem entries returns all parts of speech for a word, using a large feature-based tagset. So the word occurred is tagged with the op- tions ⟨V PCP2 SV⟩ and ⟨V PAST VFIN SV⟩, meaning it can be a participle (PCP2) for an intransitive (SV) verb, or a past (PAST) finite (VFIN) form of an intransitive (SV) verb. A set of 3,744 constraints are then applied to the input sentence to rule out parts of speech inconsistent with the context. For example here’s a rule for the ambiguous word that that eliminates all tags except the ADV (adverbial intensifier) sense (this is the sense in the sentence it isn’t that odd):
ADVERBIAL-THAT RULE Given input: “that”
if (+1 A/ADV/QUANT); /* if next word is adj, adverb, or quantifier */
(+2 SENT-LIM); /* and following which is a sentence boundary, */ (NOT -1 SVOC/A); /* and the previous word is not a verb like */
/* ‘consider’ which allows adjs as object complements */ then eliminate non-ADV tags else eliminate ADV tag
Manning (2011) investigates the remaining 2.7% of errors in a high-performing tagger, the bidirectional MEMM-style model described above (Toutanova et al., 2003). He suggests that a third or half of these remaining errors are due to errors or inconsistencies in the training data, a third might be solvable with richer linguistic models, and for the remainder the task is underspecified or unclear.
Supervised tagging relies heavily on in-domain training data hand-labeled by experts. Ways to relax this assumption include unsupervised algorithms for cluster- ing words into part-of-speech-like classes, summarized in Christodoulopoulos et al. (2010), and ways to combine labeled and unlabeled data, for example by co-training (Clark et al. 2003; Søgaard 2010).
See Householder (1995) for historical notes on parts of speech, and Sampson (1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.
Exercises
8.1 Find one tagging error in each of the following sentences that are tagged with the Penn Treebank tagset:
1. I/PRP need/VBP a/DT flight/NN from/IN Atlanta/NN
2. Does/VBZ this/DT flight/NN serve/VB dinner/NNS
3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP
4. Can/VBPyou/PRPlist/VBthe/DTnonstop/JJafternoon/NNflights/NNS

26 CHAPTER 8 8.2
•
PART-OF-SPEECH TAGGING
Use the Penn Treebank tagset to tag each word in the following sentences from Damon Runyon’s short stories. You may ignore punctuation. Some of these are quite difficult; do your best.
1. It is a nice night.
2. This crap game is over a garage in Fifty-second Street. . .
3. ...Nobodyevertakesthenewspapersshesells...
4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a
mournful voice.
5. ...IamsittinginMindy’srestaurantputtingonthegefilltefish,whichis
a dish I am very fond of, ...
6. When a guy and a doll get to taking peeks back and forth at each other,
why there you are indeed.
Now compare your tags from the previous exercise with one or two friend’s answers. On which words did you disagree the most? Why?
Implement the “most likely tag” baseline. Find a POS-tagged training set, and use it to compute for each word the tag that maximizes p(t|w). You will need to implement a simple tokenizer to deal with sentence boundaries. Start by assuming that all unknown words are NN and compute your error rate on known and unknown words. Now write at least five rules to do a better job of tagging unknown words, and show the difference in error rates.
Build a bigram HMM tagger. You will need a part-of-speech-tagged corpus. First split the corpus into a training set and test set. From the labeled training set, train the transition and observation probabilities of the HMM tagger di- rectly on the hand-tagged data. Then implement the Viterbi algorithm so that you can label an arbitrary test sentence. Now run your algorithm on the test set. Report its error rate and compare its performance to the most frequent tag baseline.
Do an error analysis of your tagger. Build a confusion matrix and investigate the most frequent errors. Propose some features for improving the perfor- mance of your tagger on these errors.
8.3 8.4
8.5
8.6

 Abney, S. P., Schapire, R. E., and Singer, Y. (1999). Boosting applied to tagging and PP attachment. In EMNLP/VLC-99, 38–45.
Bahl, L. R. and Mercer, R. L. (1976). Part of speech as- signment by a statistical decision algorithm. In Proceed- ings IEEE International Symposium on Information The- ory, 88–89.
Brants, T. (2000). TnT: A statistical part-of-speech tagger. In ANLP 2000, 224–231.
Broschart, J. (1997). Why Tongan does it differently. Lin- guistic Typology, 1, 123–165.
Charniak, E., Hendrickson, C., Jacobson, N., and Perkowitz, M. (1993). Equations for part-of-speech tagging. In AAAI- 93, 784–789. AAAI Press.
Christodoulopoulos, C., Goldwater, S., and Steedman, M. (2010). Two decades of unsupervised POS induction: How far have we come?. In EMNLP-10.
Church, K. W. (1988). A stochastic parts program and noun phrase parser for unrestricted text. In ANLP 1988, 136– 143.
Church, K. W. (1989). A stochastic parts program and noun phrase parser for unrestricted text. In ICASSP-89, 695– 698.
Clark, S., Curran, J. R., and Osborne, M. (2003). Bootstrap- ping pos taggers using unlabelled data. In CoNLL-03, 49– 55.
DeRose, S. J. (1988). Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14, 31–39.
Evans, N. (2000). Word classes in the world’s languages. In Booij, G., Lehmann, C., and Mugdan, J. (Eds.), Mor- phology: A Handbook on Inflection and Word Formation, 708–732. Mouton.
Francis, W. N. and Kucˇera, H. (1982). Frequency Analysis of English Usage. Houghton Mifflin, Boston.
Garside, R. (1987). The CLAWS word-tagging system. In Garside, R., Leech, G., and Sampson, G. (Eds.), The Com- putational Analysis of English, 30–41. Longman.
Garside, R., Leech, G., and McEnery, A. (1997). Corpus Annotation. Longman.
Gil, D. (2000). Syntactic categories, cross-linguistic varia- tion and universal grammar. In Vogel, P. M. and Comrie, B. (Eds.), Approaches to the Typology of Word Classes, 173– 216. Mouton.
Greene, B. B. and Rubin, G. M. (1971). Automatic grammat- ical tagging of English. Department of Linguistics, Brown University, Providence, Rhode Island.
Hajicˇ, J. (2000). Morphological tagging: Data vs. dictionar- ies. In NAACL 2000. Seattle.
Hakkani-Tu ̈r, D., Oflazer, K., and Tu ̈r, G. (2002). Statistical morphological disambiguation for agglutinative languages. Journal of Computers and Humanities, 36(4), 381–410.
Harris, Z. S. (1962). String Analysis of Sentence Structure. Mouton, The Hague.
Householder, F. W. (1995). Dionysius Thrax, the technai, and Sextus Empiricus. In Koerner, E. F. K. and Asher, R. E. (Eds.), Concise History of the Language Sciences, 99–103. Elsevier Science.
Exercises 27
Jelinek, F. and Mercer, R. L. (1980). Interpolated estimation of Markov source parameters from sparse data. In Gelsema, E. S. and Kanal, L. N. (Eds.), Proceedings, Workshop on Pattern Recognition in Practice, 381–397. North Holland.
Joshi, A. K. and Hopely, P. (1999). A parser from antiq- uity. In Kornai, A. (Ed.), Extended Finite State Models of Language, 6–15. Cambridge University Press.
Karlsson, F., Voutilainen, A., Heikkila ̈, J., and Anttila, A. (Eds.). (1995). Constraint Grammar: A Language- Independent System for Parsing Unrestricted Text. Mouton de Gruyter.
Karttunen, L. (1999). Comments on Joshi. In Kornai, A. (Ed.), Extended Finite State Models of Language, 16–18. Cambridge University Press.
Klein, S. and Simmons, R. F. (1963). A computational ap- proach to grammatical coding of English words. Journal of the Association for Computing Machinery, 10(3), 334–347.
Kupiec, J. (1992). Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6, 225–242.
Lafferty, J. D., McCallum, A., and Pereira, F. C. N. (2001). Conditional random fields: Probabilistic models for seg- menting and labeling sequence data. In ICML 2001.
Manning, C. D. (2011). Part-of-speech tagging from 97% to 100%: Is it time for some linguistics?. In CICLing 2011, 171–189.
Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2), 313– 330.
Marshall, I. (1983). Choice of grammatical word-class with- out GLobal syntactic analysis: Tagging words in the LOB corpus. Computers and the Humanities, 17, 139–150.
Marshall, I. (1987). Tag selection using probabilistic meth- ods. In Garside, R., Leech, G., and Sampson, G. (Eds.), The Computational Analysis of English, 42–56. Longman.
Merialdo, B. (1994). Tagging English text with a probabilis- tic model. Computational Linguistics, 20(2), 155–172.
Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajicˇ, J., Manning, C. D., McDonald, R., Petrov, S., Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D. (2016). Universal Dependencies v1: A multilingual treebank collection. In LREC.
Oravecz, C. and Dienes, P. (2002). Efficient stochastic part- of-speech tagging for Hungarian. In LREC-02, 710–717.
Ratnaparkhi, A. (1996). A maximum entropy part-of-speech tagger. In EMNLP 1996, 133–142.
Sampson, G. (1987). Alternative grammatical coding sys- tems. In Garside, R., Leech, G., and Sampson, G. (Eds.), The Computational Analysis of English, 165–183. Long- man.
Samuelsson, C. (1993). Morphological tagging based en- tirely on Bayesian inference. In 9th Nordic Conference on Computational Linguistics NODALIDA-93. Stockholm.
Schu ̈tze, H. and Singer, Y. (1994). Part-of-speech tagging using a variable memory Markov model. In ACL-94, 181– 187.
Søgaard, A. (2010). Simple semi-supervised training of part- of-speech taggers. In ACL 2010, 205–208.

28 Chapter 8 • Part-of-Speech Tagging
Stolz, W. S., Tannenbaum, P. H., and Carstensen, F. V. (1965). A stochastic approach to the grammatical coding of English. CACM, 8(6), 399–405.
Thede, S. M. and Harper, M. P. (1999). A second-order hid- den Markov model for part-of-speech tagging. In ACL-99, 175–182.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In HLT-NAACL-03.
Tseng, H., Jurafsky, D., and Manning, C. D. (2005). Mor- phological features help POS tagging of unknown words across language varieties. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing.
Voutilainen, A. (1995). Morphological disambiguation. In Karlsson, F., Voutilainen, A., Heikkila ̈, J., and Anttila, A. (Eds.), Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text, 165–284. Mouton de Gruyter.
Voutilainen, A. (1999). Handcrafted rules. In van Halteren, H. (Ed.), Syntactic Wordclass Tagging, 217–246. Kluwer.
Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L. A., and Palmucci, J. (1993). Coping with ambiguity and un- known words through probabilistic models. Computational Linguistics, 19(2), 359–382.